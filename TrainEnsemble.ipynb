{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask-expr in /opt/conda/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: dask==2024.4.0 in /opt/conda/lib/python3.10/site-packages (from dask-expr) (2024.4.0)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in /opt/conda/lib/python3.10/site-packages (from dask-expr) (12.0.1)\n",
      "Requirement already satisfied: pandas>=2 in /opt/conda/lib/python3.10/site-packages (from dask-expr) (2.1.4)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.4.0->dask-expr) (6.10.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask-expr) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask-expr) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask-expr) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask-expr) (2024.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2024.4.0->dask-expr) (3.17.0)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.10/site-packages (from partd>=1.2.0->dask==2024.4.0->dask-expr) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2->dask-expr) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "!pip install dask-expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "zd = zstd.ZstdDecompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(model_path, file):\n",
    "    try:\n",
    "        zd = zstd.ZstdDecompressor()\n",
    "        return pickle.loads(zd.decompress(open(model_path + file, 'rb').read()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return pd.Series([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPreds(dataset, location):\n",
    "    model_path = 'nn1/{}_{}/'.format(dataset, location.replace(' ', '_'))\n",
    "    files = sorted([f for f in os.listdir(model_path) if 'ckpt' not in f\n",
    "                       and '9-' in f\n",
    "                   ])\n",
    "    print(len(files), 'files for {}-{}'.format(dataset, location))\n",
    "    \n",
    "    r = Parallel(os.cpu_count() * 2)(delayed(loadFile)(model_path, file) for file in files)\n",
    "    all_preds = list(zip(files, r))    \n",
    "    \n",
    "    all_val_preds = defaultdict(list)\n",
    "    all_test_preds = defaultdict(list)\n",
    "    for file, preds in all_preds:\n",
    "        m = file.split('_run')[0]\n",
    "        e = file.split('epoch=')[-1].split('-')[0]\n",
    "        mstr = m + '_epoch{}'.format(e)\n",
    "        vt = file.split('.pkl')[0].split('-')[-1]\n",
    "        if vt == 'val':\n",
    "            all_val_preds[mstr].append(preds)\n",
    "        elif vt == 'test' and '_full' in file:\n",
    "            all_test_preds[mstr].append(preds)\n",
    "    val_preds = {}; test_preds = {}\n",
    "    for k, v in all_val_preds.items():\n",
    "        v = pd.concat(v)\n",
    "        v = v.groupby(v.index).mean()\n",
    "        val_preds[k] = v\n",
    "    for k, v in all_test_preds.items():\n",
    "        v = pd.concat(v)\n",
    "        v = v.groupby(v.index).mean()\n",
    "        test_preds[k] = v\n",
    "        \n",
    "    print(len(test_preds))\n",
    "    print(len(val_preds))\n",
    "    \n",
    "    test_preds = pd.DataFrame(test_preds)#[val_preds.columns]\n",
    "    val_preds = pd.DataFrame(val_preds)[test_preds.columns]\n",
    "    return test_preds, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open('cache/all_data_{}.pkl'.format(dataset), 'rb'))\n",
    "submission = pickle.load(open('cache/submission_{}.pkl'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_files = [f for f in os.listdir('clfs_{}'.format(dataset)) \n",
    "                 if 'lgb' in f]\n",
    "lgb_preds = pickle.load(open('clfs_{}/'.format(dataset) + lgb_files[0], 'rb'))[1]\n",
    "lgb_preds = pd.concat(lgb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_preds.corrwith(y).sort_values()[::-1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(wts, x, y, reg = 1, l1_ratio = 0):\n",
    "    wts = wts /wts.sum() #/ max(wts.sum() ** 0.5, 1.0)#wts.sum() * 0.9\n",
    "    blend = ( x * wts[None, :]).sum(axis = 1)\n",
    "    return ( \n",
    "        mean_squared_error(y, blend)\n",
    "            + reg *( (wts ** 2).sum() + l1_ratio * np.abs(wts).sum()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(x, y, reg = 1, l1_ratio = 0, tol = 1e-4 ):\n",
    "    wts = scipy.optimize.minimize(\n",
    "    score, np.ones(x.shape[1]) / x.shape[1],#len(x.columns), \n",
    "        tol = tol,\n",
    "    args=(x, y, reg, l1_ratio), \n",
    "    bounds=[(0, 1) for i in range(x.shape[1])],#len(x.columns))],\n",
    "    ).x\n",
    "    return wts / wts.sum()# ** 0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLR(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, reg = 1.0, l1_ratio = 0, tol = 1e-4):\n",
    "        self.reg = reg\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.classes_ = np.array((0, 1))\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        wts = optimize(X.values, y.values, \n",
    "                           self.reg, self.l1_ratio, self.tol)\n",
    "        self.wts = wts #/ max(wts.sum(), 1)# * 0.9\n",
    "        # print(self.wts.sum())\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (X * self.wts).sum(axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_params = {'reg': [ 1e-4, 3e-3, 1e-3, 3e-3, \n",
    "                      0.01, 0.03, 0.1, 0.3, 1, 3,  ],\n",
    "               'l1_ratio': [ 0, 0.01, 0.03, 0.1, 0.2, 0.5, ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold():\n",
    "    def __init__(self, n_splits=5, gap = 30):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "        \n",
    "    def get_n_splits(self, X, y = None, groups = None): return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = groups.sort_values()\n",
    "        X = X.reindex(groups.index)# sort_values(groups)\n",
    "        y = y.reindex(X.index);\n",
    "                     \n",
    "        X, y, groups = sklearn.utils.indexable(X, y, groups)\n",
    "        indices = np.arange(len(X))\n",
    "        \n",
    "        n_splits = self.n_splits\n",
    "        for i in range(n_splits):\n",
    "            test = indices[ i * len(X) // n_splits: (i + 1) * len(X) // n_splits ]#.index\n",
    "            train = indices[ (groups <= groups.iloc[test].min() - datetime.timedelta(days = self.gap) )\n",
    "                          | (groups >= groups.iloc[test].max() + datetime.timedelta(days = self.gap) ) ]#.index\n",
    "            yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedPurgedKFold():\n",
    "    def __init__(self, n_splits = 5, n_repeats = 1, gap = None):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_repeats = n_repeats\n",
    "        self.gap = gap\n",
    "        \n",
    "    def get_n_splits(self, X, y = None, groups = None): \n",
    "        return self.n_splits * self.n_repeats + self.n_repeats * ( self.n_repeats - 1) // 2\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for i in range(self.n_repeats):\n",
    "            for f in PurgedKFold(self.n_splits + i, gap = self.gap if self.gap else None).split(X, y, groups):\n",
    "                yield f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 files for tg-Los Angeles (SoCAB)\n",
      "4\n",
      "4\n",
      "0.8577508092200263\n"
     ]
    }
   ],
   "source": [
    "# for location in ['Delhi', 'Los Angeles (SoCAB)', 'Taipei']:    \n",
    "for location in ['Los Angeles (SoCAB)']:\n",
    "    test_preds, val_preds = loadPreds('tg', location)\n",
    "    y = all_data.value.reindex(val_preds.index)\n",
    "    g = all_data.datetime.reindex(y.index)\n",
    "    # print(mean_squared_error(y, val_preds.mean(axis = 1)))\n",
    "    print(np.corrcoef(val_preds.mean(axis = 1), y )[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127618434959551\n",
      "\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.01, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.01, reg=0.0001)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.2, reg=0.0001)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.1, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(reg=3)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.1, reg=0.003)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.03, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.1, reg=0.003)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.01, reg=0.001)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.03, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.2, reg=1)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.03, reg=0.003)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.1, reg=0.003)\n",
      "0.907089855812681\n",
      "CLR(reg=0.01)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.01, reg=0.001)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.5, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.1, reg=3)\n",
      "0.9121601131991063\n",
      "CLR(l1_ratio=0.2, reg=0.01)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.1, reg=0.001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.1, reg=0.03)\n",
      "0.9121601131991063\n",
      "CLR(reg=1)\n",
      "0.9176822256131307\n",
      "CLR(l1_ratio=0.01, reg=0.0001)\n",
      "0.907089855812681\n",
      "CLR(l1_ratio=0.2, reg=0.001)\n",
      "0.9121601131991063\n",
      "CLR(reg=0.03)\n",
      "\n",
      "0.9127618434959553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    lgb_val_preds = lgb_preds.groupby(lgb_preds.index).mean()\n",
    "    lgb_val_preds = lgb_val_preds.reindex(y.index)\n",
    "    lgb_val_preds\n",
    "    lgb_test_preds = pd.read_csv('submissions_{}/new.csv'.format(dataset))\n",
    "    submission = lgb_test_preds\n",
    "    lgb_test_preds = lgb_test_preds.value.reindex(test_preds.index)\n",
    "    #print(mean_squared_error(y, lgb_val_preds ))\n",
    "    print(np.corrcoef(lgb_val_preds, y)[0, 1])\n",
    "    print();\n",
    "    for i in range(10):\n",
    "        val_preds['lgb{}'.format(i)] = lgb_val_preds\n",
    "        test_preds['lgb{}'.format(i)] = lgb_test_preds\n",
    "    enet_val_preds = []\n",
    "    enet_test_preds = []\n",
    "    all_coefs = []\n",
    "    for i in range(8):\n",
    "        folds = PurgedKFold( random.randrange(4, 7)\n",
    "                                        if dataset == 'pm' else \n",
    "                                        random.randrange(3, 4),\n",
    "                                    gap = random.randrange(20, 40)).split(\n",
    "                    val_preds, y, g)\n",
    "        for train_fold, test_fold in folds:\n",
    "            # l = random.randrange(0, len(train_fold)//random.randrange(5, 20))\n",
    "            # s = random.randrange(0, len(train_fold) - l)\n",
    "            # train_fold = train_fold[:s].tolist() + train_fold[s + l:].tolist()\n",
    "            vp = val_preds.copy()\n",
    "\n",
    "            #Takes a sample\n",
    "            filtered_columns = [c for c in list(val_preds.columns) if 'lgb' not in c]\n",
    "            k = int((0.4 + 0.2 * random.random()) * len(filtered_columns))\n",
    "            k = max(k, len(filtered_columns))\n",
    "            #print(len(filtered_columns))\n",
    "            #print(k)\n",
    "            model_drops = random.sample(filtered_columns, k=4)\n",
    "\n",
    "            vp.loc[:, model_drops] = 0\n",
    "            model = RandomizedSearchCV(\n",
    "                CLR(#tol = 3e-3,\n",
    "                   ),  clr_params, cv = RepeatedPurgedKFold( \n",
    "                                            random.randrange(4, 7)\n",
    "                                                    if dataset == 'pm' else \n",
    "                                                    random.randrange(3, 6),\n",
    "                                            random.randrange(1, 3),\n",
    "                                                 gap = random.randrange(20, 60)),\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                random_state = datetime.datetime.now().microsecond,\n",
    "                        n_iter = 4, n_jobs = -1,\n",
    "            )\n",
    "            model.fit(vp.iloc[train_fold], y.iloc[train_fold], \n",
    "                          groups = g.iloc[train_fold])\n",
    "            enet_val_preds.append(pd.Series(\n",
    "                model.predict(val_preds.iloc[test_fold]), \n",
    "                                  val_preds.index[test_fold]))\n",
    "            enet_test_preds.append(pd.Series(\n",
    "                model.predict(test_preds), test_preds.index))\n",
    "            enet_val_preds[-1]\n",
    "            print(np.corrcoef(        enet_val_preds[-1], y.iloc[test_fold])[0, 1])\n",
    "            clf = model.best_estimator_\n",
    "            print(clf)\n",
    "            all_coefs.append(pd.Series(clf.wts, val_preds.columns))\n",
    "    \n",
    "    all_coefs = pd.concat(all_coefs)\n",
    "    all_coefs = all_coefs.groupby(all_coefs.index).mean()\n",
    "    \n",
    "    enet_val_preds = (val_preds * all_coefs).sum(axis = 1)\n",
    "    enet_test_preds = (test_preds * all_coefs).sum(axis = 1)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print(np.corrcoef(y, enet_val_preds)[0, 1])\n",
    "    print()\n",
    "    os.makedirs('stack1', exist_ok = True)\n",
    "    pickle.dump(all_coefs, \n",
    "                open('stack1/{}_{}.pkl'.format(dataset, location), 'wb'))\n",
    "    submission = submission.reindex(enet_test_preds.index)\n",
    "    submission.value = enet_test_preds\n",
    "    os.makedirs('submissions_{}/nn1'.format(dataset), exist_ok = True)\n",
    "    submission.to_csv('submissions_{}/nn1/{}.csv'.format(\n",
    "                            dataset, location.replace(' ', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([pd.read_csv('submissions_{}/nn1/'.format(\n",
    "                        dataset) + file, index_col = 0)\n",
    "            for file in os.listdir('submissions_{}/nn1/'.format(\n",
    "                        dataset,))]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('submissions_{}/stack1.csv'.format(dataset), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../submissions_tg/stack1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prior \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../submissions_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/stack1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, index_col = 0)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../submissions_tg/stack1.csv'"
     ]
    }
   ],
   "source": [
    "prior = pd.read_csv('../submissions_{}/stack1.csv'.format(dataset))#, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.corrcoef(prior.value, combined.value)[0, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
