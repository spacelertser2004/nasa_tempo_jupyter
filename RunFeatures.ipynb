{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Aiding installations\n",
    "- Website links continuously change, please make sure the link adress is correct.\n",
    "- NCAR's RDA UCAR website sometimes have planned outtages that prevent installations.\n",
    "\n",
    "Updates to base model from NASA Airathon Competition\n",
    "- Sections are adding for each segment of the installation (GFS, IFS, ASSIM, #TROPOMI, TEMPO)\n",
    "- Code has been cleaned up and easily followed throughout the download process.\n",
    "- If a file has already been downloaded into, for example 'inference/asssim/', the file will be skipped in the loadAssim function to save redundant downloading and processing time.\n",
    "- module pip/mamba installation tips are added to aid setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip instal -r requirements.txt\n",
    "# !pip install dask-expr\n",
    "# !pip install opencv-python\n",
    "# !pip install xarray\n",
    "# !pip install joblib\n",
    "# !pip install pydap\n",
    "# # !pip install pyhdf\n",
    "# !pip install basemap\n",
    "# # # !pip install pygrib\n",
    "\n",
    "# mamba install pyhdf\n",
    "# mamba install pygrib\n",
    "# mamba install netCDF4\n",
    "# mamba install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import zstandard as zstd\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import botocore\n",
    "# from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "import pydap\n",
    "import xarray as xr\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files/Convert csv files to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure = dict([e.split('=') for e in open('secure.txt', 'r').read().split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = dict([e.split('=') for e in open('infer.txt', 'r').read().split('\\n')])\n",
    "# infer = {k: v.split(',') for k, v in infer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data_{}/train_labels.csv'.format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.concat( (\n",
    "        pd.read_csv('data_tg/grid_metadata.csv'),\n",
    ") ).drop_duplicates().reset_index(drop = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data_{}/submission_format.csv'.format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.read_csv('data_{}/{}_satellite_metadata{}.csv'.format(\n",
    "                    dataset, *(('pm25', '') if dataset == 'pm' \n",
    "                                else ('no2', '_0AF3h09'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.time_end = pd.to_datetime(files.time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['location'] = grid.set_index('grid_id')['location'].reindex(labels.grid_id).values\n",
    "labels['datetime'] = pd.to_datetime(labels.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['location'] = grid.set_index('grid_id').location.reindex(submission.grid_id).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities & Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities = {\n",
    "#  'Taipei': ( (121.5, 121.5), (25.0, 25) ),\n",
    "#  'Delhi': ( (77.0, 77.25), (28.75, 28.5) ),\n",
    "#  'LA': ((360-118.25, 360-117.75), (34.0, 34.0) )\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {\n",
    " 'LA': ((360-118.25, 360-117.75), (34.0, 34.0) ) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities2 = {\n",
    "#  'tpe': ( 121.5, 25 ),\n",
    "#  'dl': ( 77.0, 28.5 ),\n",
    "#  'la': (-118.25, 34.0 ) \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = {\n",
    " 'la': (-118.25, 34.0 ) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = {'la': [('3A3IE', -117.9114, 34.1494),\n",
    "#   ('3S31A', -117.9563, 33.8142),\n",
    "#   ('7II4T', -118.0461, 34.0006),\n",
    "#   ('8BOQH', -118.4504, 34.0379),\n",
    "#   ('A2FBI', -117.4173, 34.0006),\n",
    "#   ('A5WJI', -117.9563, 33.9261),\n",
    "#   ('B5FKJ', -117.5071, 34.1123),\n",
    "#   ('C8HH7', -116.519, 33.8516),\n",
    "#   ('DHO4M', -118.3605, 34.1866),\n",
    "#   ('DJN0F', -117.6419, 34.1123),\n",
    "#   ('E5P9N', -117.5071, 34.0006),\n",
    "#   ('FRITQ', -118.1809, 33.8516),\n",
    "#   ('H96P6', -118.5402, 34.1866),\n",
    "#   ('HUZ29', -117.2825, 34.1123),\n",
    "#   ('I677K', -117.5071, 34.0751),\n",
    "#   ('IUON3', -117.7317, 34.0751),\n",
    "#   ('JNUQF', -118.2258, 33.8142),\n",
    "#   ('PG3MI', -118.2258, 34.0751),\n",
    "#   ('QH45V', -118.4504, 33.9634),\n",
    "#   ('QJHW4', -118.5402, 34.3722),\n",
    "#   ('QWDU8', -118.1359, 34.1494),\n",
    "#   ('VBLD0', -118.2258, 33.8888),\n",
    "#   ('VDUTN', -117.9114, 33.8142),\n",
    "#   ('WT52R', -116.8783, 33.9261),\n",
    "#   ('X5DKW', -117.597, 34.0379),\n",
    "#   ('Z0VWC', -118.1809, 33.7769),\n",
    "#   ('ZP1FZ', -117.8665, 34.1494),\n",
    "#   ('ZZ8JF', -117.3275, 33.6648)],\n",
    "#  'tpe': [('1X116', 121.5033, 24.998),\n",
    "#   ('90BZ1', 121.5482, 25.0387),\n",
    "#   ('9Q6TA', 121.5482, 25.0794),\n",
    "#   ('KW43U', 121.5931, 25.0387),\n",
    "#   ('VR4WG', 121.5033, 25.0794),\n",
    "#   ('XJF9O', 121.5033, 25.0387),\n",
    "#   ('XNLVD', 121.5033, 25.1201)],\n",
    "#  'dl': [('1Z2W7', 77.2821, 28.5664),\n",
    "#   ('6EIL6', 77.0575, 28.5664),\n",
    "#   ('7334C', 77.1024, 28.5664),\n",
    "#   ('78V83', 76.9227, 28.5664),\n",
    "#   ('7F1D1', 77.1024, 28.6058),\n",
    "#   ('8KNI6', 77.2821, 28.4874),\n",
    "#   ('90S79', 77.1922, 28.6452),\n",
    "#   ('A7UCQ', 77.2372, 28.6058),\n",
    "#   ('AZJ0Z', 77.2372, 28.724),\n",
    "#   ('C7PGV', 77.1922, 28.5269),\n",
    "#   ('CPR0W', 77.2821, 28.6846),\n",
    "#   ('D72OT', 77.1473, 28.724),\n",
    "#   ('D7S1G', 77.327, 28.6846),\n",
    "#   ('E2AUK', 77.0126, 28.6058),\n",
    "#   ('GAC6R', 77.1024, 28.7634),\n",
    "#   ('GJLB2', 77.1024, 28.4874),\n",
    "#   ('GVQXS', 77.1922, 28.6846),\n",
    "#   ('HANW9', 77.1922, 28.5664),\n",
    "#   ('HM74A', 77.1024, 28.6846),\n",
    "#   ('IUMEZ', 77.2372, 28.6452),\n",
    "#   ('KZ9W9', 77.1473, 28.6452),\n",
    "#   ('NE7BV', 77.1024, 28.8421),\n",
    "#   ('P8JA5', 77.2372, 28.5664),\n",
    "#   ('PJNW1', 77.1922, 28.724),\n",
    "#   ('PW0JT', 76.9227, 28.6846),\n",
    "#   ('S77YN', 77.0575, 28.724),\n",
    "#   ('SZLMT', 77.1473, 28.6846),\n",
    "#   ('UC74Z', 77.2821, 28.5269),\n",
    "#   ('VXNN3', 77.1473, 28.8028),\n",
    "#   ('VYH7U', 77.0575, 28.7634),\n",
    "#   ('WZNCR', 77.1473, 28.5664),\n",
    "#   ('YHOPV', 77.2821, 28.6452),\n",
    "#   ('ZF3ZW', 77.0575, 28.6846)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {'la': [('3A3IE', -117.9114, 34.1494),\n",
    "  ('3S31A', -117.9563, 33.8142),\n",
    "  ('7II4T', -118.0461, 34.0006),\n",
    "  ('8BOQH', -118.4504, 34.0379),\n",
    "  ('A2FBI', -117.4173, 34.0006),\n",
    "  ('A5WJI', -117.9563, 33.9261),\n",
    "  ('B5FKJ', -117.5071, 34.1123),\n",
    "  ('C8HH7', -116.519, 33.8516),\n",
    "  ('DHO4M', -118.3605, 34.1866),\n",
    "  ('DJN0F', -117.6419, 34.1123),\n",
    "  ('E5P9N', -117.5071, 34.0006),\n",
    "  ('FRITQ', -118.1809, 33.8516),\n",
    "  ('H96P6', -118.5402, 34.1866),\n",
    "  ('HUZ29', -117.2825, 34.1123),\n",
    "  ('I677K', -117.5071, 34.0751),\n",
    "  ('IUON3', -117.7317, 34.0751),\n",
    "  ('JNUQF', -118.2258, 33.8142),\n",
    "  ('PG3MI', -118.2258, 34.0751),\n",
    "  ('QH45V', -118.4504, 33.9634),\n",
    "  ('QJHW4', -118.5402, 34.3722),\n",
    "  ('QWDU8', -118.1359, 34.1494),\n",
    "  ('VBLD0', -118.2258, 33.8888),\n",
    "  ('VDUTN', -117.9114, 33.8142),\n",
    "  ('WT52R', -116.8783, 33.9261),\n",
    "  ('X5DKW', -117.597, 34.0379),\n",
    "  ('Z0VWC', -118.1809, 33.7769),\n",
    "  ('ZP1FZ', -117.8665, 34.1494),\n",
    "  ('ZZ8JF', -117.3275, 33.6648)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDict(d):\n",
    "    return {k: cleanDict(v) for k, v in d.items() } if isinstance(d, defaultdict) else d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\n",
    "    #  (6, 'Maximum/Composite radar reflectivity:dB (instant):regular_ll:atmosphere:level 0', ),\n",
    "    #  (7, 'Visibility:m (instant):regular_ll:surface:level 0', ),\n",
    "     (11, 'Wind speed (gust):m s**-1 (instant):regular_ll:surface:level 0', ),\n",
    "(402, 'Surface pressure:Pa (instant):regular_ll:surface:level 0'),\n",
    "# (404, 'Temperature:K (instant):regular_ll:surface:level 0'),\n",
    "# (405, 'Soil Temperature:K (instant):regular_ll:depthBelowLandLayer:levels 0.0-0.1 m'),\n",
    "(406, 'Volumetric soil moisture content:Proportion (instant):regular_ll:depthBelowLandLayer:levels 0.0-0.1 m'),\n",
    "(415, '2 metre temperature:K (instant):regular_ll:heightAboveGround:level 2 m'),\n",
    "(416, '2 metre specific humidity:kg kg**-1 (instant):regular_ll:heightAboveGround:level 2 m'),\n",
    "# (417, '2 metre dewpoint temperature:K (instant):regular_ll:heightAboveGround:level 2 m:'),#fcst time 0 hrs:from 202001010000\n",
    "(418, '2 metre relative humidity:% (instant):regular_ll:heightAboveGround:level 2 m:'), #fcst time 0 hrs:from 202001010000\n",
    "(419, 'Apparent temperature:K (instant):regular_ll:heightAboveGround:level 2 m:'),#fcst time 0 hrs:from 202001010000\n",
    "(420, '10 metre U wind component:m s**-1 (instant):regular_ll:heightAboveGround:level 10 m:'),#fcst time 0 hrs:from 202001010000\n",
    "(421, '10 metre V wind component:m s**-1 (instant):regular_ll:heightAboveGround:level 10 m:'),#fcst time 0 hrs:from 202001010000\n",
    "# (435, 'Precipitable water:kg m**-2 (instant):regular_ll:atmosphereSingleLayer:level 0 considered as a single layer'),#:fcst time 0 hrs:from 202001010000\n",
    "(436, 'Cloud water:kg m**-2 (instant):regular_ll:atmosphereSingleLayer:level 0 considered as a single layer:'),#fcst time 0 hrs:from 202001010000\n",
    "(437, 'Relative humidity:% (instant):regular_ll:atmosphereSingleLayer:level 0 considered as a single layer:'),#fcst time 0 hrs:from 202001010000\n",
    "(438, 'Total ozone:DU (instant):regular_ll:atmosphereSingleLayer:level 0 considered as a single layer:'),#fcst time 0 hrs:from 202001010000        \n",
    "    # (424,  'Precipitation rate:kg m**-2 s**-1 (instant):regular_ll:surface:level 0'),\n",
    "    # (484, 'Temperature:K (instant):regular_ll:pressureFromGroundLayer', ),\n",
    "    # (485, 'Relative humidity:% (instant):regular_ll:pressureFromGroundLayer:levels 3000-0 Pa', ),\n",
    "    # (486, 'Specific humidity:kg kg**-1 (instant):regular_ll:pressureFromGroundLayer:levels 3000-0 Pa', ),\n",
    "    # (487, 'U component of wind:m s**-1 (instant):regular_ll:pressureFromGroundLayer:levels 3000-0 Pa', ),\n",
    "    # (488, 'V component of wind:m s**-1 (instant):regular_ll:pressureFromGroundLayer:levels 3000-0 Pa', ),\n",
    "    # (520, 'Pressure reduced to MSL:Pa (instant):regular_ll:meanSea:level 0:', ),        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processGFS(file, d):\n",
    "    p = pygrib.open(file)\n",
    "    lat, lon = p[1].latlons()\n",
    "    spots = {}\n",
    "    for city, ( (lonmin, lonmax) , (latmin, latmax) ) in cities.items():\n",
    "        xmin = np.argmax( (lat == latmin).sum(axis = 1)  )#[0]\n",
    "        xmax = np.argmax( (lat == latmax).sum(axis = 1)  )#[0]\n",
    "        ymin = np.argmax( (lon == lonmin).sum(axis = 0)  )#[0]\n",
    "        ymax = np.argmax( (lon == lonmax).sum(axis = 0)  )#[0]\n",
    "        spots[city] = ((xmin, xmax), (ymin, ymax))\n",
    "    data = []\n",
    "    for e in p:\n",
    "        if any(z in str(e) for i, z in feats): \n",
    "            arr = e.values\n",
    "            assert arr.shape == lat.shape\n",
    "            for spot, ((xmin, xmax), (ymin, ymax)) in spots.items():\n",
    "                data.append( (str(e), \n",
    "                                spot,\n",
    "                ((lat[xmin - d :xmax + 1 + d, ymin - d :ymax + 1 + d].min(),\n",
    "                  lat[xmin - d :xmax + 1 + d, ymin - d :ymax + 1 + d].max()),\n",
    "                 (lon[xmin - d :xmax + 1 + d, ymin - d :ymax + 1 + d].min(),\n",
    "                  lon[xmin - d :xmax + 1 + d, ymin - d :ymax + 1 + d].max())),\n",
    "                \n",
    "                arr[xmin - d :xmax + 1 + d, ymin - d :ymax + 1 + d].astype(np.float32),\n",
    "                \n",
    "                arr[xmin:xmax + 1, ymin:ymax + 1].mean() ) );\n",
    "                # if len(data) == 1: print(data)\n",
    "                # print(data); return data\n",
    "    return data\n",
    "    # break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullGFS(files):\n",
    "    results = []\n",
    "    # for i in range(1):\n",
    "    #     try:\n",
    "    #         pswd = secure['password']\n",
    "    #         values = {'email' : secure['username'], 'passwd' : pswd, 'action' : 'login'}\n",
    "    #         login_url = 'https://rda.ucar.edu/accounts/orcid/login/?process=login'\n",
    "\n",
    "    #         ret = requests.post(login_url, data=values)\n",
    "    #         if ret.status_code != 200:\n",
    "    #             print('Bad Authentication'); time.sleep(i); continue;\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "    #         time.sleep(i)\n",
    "                \n",
    "    #print(filelist); return;\n",
    "    dspath = 'https://thredds.rda.ucar.edu/thredds/fileServer/files/g/ds084.1/'\n",
    "    save_dir = '/tmp/'\n",
    "        \n",
    "    zc = zstd.ZstdCompressor(level = 9)\n",
    "    os.makedirs('inference/gfs-5/', exist_ok = True)\n",
    "    processed_dir = 'inference/gfs-5/'\n",
    "    print('Downloading {} gfs files'.format(len(files)))    \n",
    "    for file in files:\n",
    "        start = time.time()\n",
    "        filename = os.path.basename(file)\n",
    "        outfile = os.path.join(processed_dir, filename)\n",
    "        processed_file = os.path.join(processed_dir, filename)\n",
    "\n",
    "        # Check if the file has already been processed\n",
    "        if os.path.exists(processed_file):\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                filename = dspath + file\n",
    "                outfile = save_dir + os.path.basename(filename)\n",
    "                print('Downloading', filename)            \n",
    "                with requests.get(filename, allow_redirects = True, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(outfile, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=1024*1024): \n",
    "                            f.write(chunk)\n",
    "            \n",
    "                s = os.path.getsize(outfile); \n",
    "                data = processGFS(outfile, 5)\n",
    "                os.remove(outfile)\n",
    "                pkl = pickle.dumps(data)\n",
    "                compr = zc.compress(pkl)\n",
    "                os.makedirs('inference/gfs-5/', exist_ok = True)\n",
    "                with open('inference/gfs-5/{}'.format(os.path.basename(filename)), 'wb') as f:\n",
    "                    f.write(compr)\n",
    "                results.append({\n",
    "                    # 'statusCode': 200,\n",
    "                    'file': os.path.basename(filename),\n",
    "                    'body': s/1e6, #os.path.getsize(outfile), #json.dumps('Hello from Lambda!'),\n",
    "                    'outlen': len(pkl),#len(pickle.dumps(data)),\n",
    "                    'outlen-compr': len(compr),#zc.compress(pickle.dumps(data))),\n",
    "                    'elaspsed_time': round(time.time() - start, 1)\n",
    "                    # 'data': json.dumps(data),\n",
    "                }); break;\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(i)\n",
    "                try: os.remove(outfile)\n",
    "                except: pass;\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listGFSFiles(dates):\n",
    "    filelist = []; fwd = 0\n",
    "    for t in dates:\n",
    "        dt = t.strftime('%Y%m%d')\n",
    "        for hr in [0, 6, 12, 18]:\n",
    "            filelist.append('{}/{}/gfs.0p25.{}{:02d}.f{:03d}.grib2'.format(\n",
    "                    dt[:4], dt, dt, hr, fwd))\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifs_tags = ['128_057_uvb',\n",
    " '128_134_sp',\n",
    " '128_136_tcw',\n",
    " '128_137_tcwv',\n",
    " '128_146_sshf',\n",
    " '128_147_slhf',\n",
    " '128_164_tcc',\n",
    " '128_165_10u',\n",
    " '128_166_10v',\n",
    " '128_167_2t',\n",
    " '128_168_2d',\n",
    " '128_169_ssrd',\n",
    " '128_175_strd',\n",
    " '128_176_ssr',\n",
    " '128_177_str',\n",
    " '128_189_sund',\n",
    " '128_206_tco3',\n",
    " '128_228_tp',\n",
    " '128_243_fal',\n",
    " '128_244_fsr',\n",
    " '128_245_flsr',\n",
    " '228_246_100u',\n",
    " '228_247_100v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processIFS(file):\n",
    "    dataset = xr.open_dataset(file, engine='netcdf4')\n",
    "    vars = list(dataset.variables)\n",
    "    assert len(vars) == 5 if 'oper.an' in file else 6 if 'oper.fc' in file else -1;\n",
    "    # assert vars[-4:] == ['latitude', 'longitude', 'time', 'utc_date']\n",
    "    \n",
    "    field = vars[0]\n",
    "    name = dataset.variables[field].attrs['long_name']\n",
    "    # print(name)     \n",
    "    clean_name = name.lower().replace(' ', '_').replace('-', '_')\n",
    "    # print(clean_name)\n",
    "    \n",
    "    sat_data = defaultdict(lambda: defaultdict(dict))\n",
    "    for location, (clon, clat) in cities2.items():\n",
    "        minimum_latitude = clat + 8\n",
    "        minimum_longitude = (clon - 10 ) % 360\n",
    "        maximum_latitude = clat - 8\n",
    "        maximum_longitude = (clon + 10) % 360\n",
    "    \n",
    "        data = dataset[field].loc[{\n",
    "                                           'latitude':slice(minimum_latitude,maximum_latitude),\n",
    "                                           'longitude':slice(minimum_longitude,maximum_longitude)}]\n",
    "        # print(data.shape)\n",
    "        \n",
    "        \n",
    "        a = data\n",
    "        \n",
    "        v = a.values\n",
    "        lat = np.tile( np.stack([a['latitude']], axis = 1), ( 1, v.shape[-1]))\n",
    "        lon = np.tile( np.stack([a['longitude']], axis = 0), ( v.shape[-2], 1))\n",
    "        assert v.shape == (4, 227, 285) if 'oper.an' in file else (2, 2, 227, 285) if 'oper.fc' in file else None\n",
    "        if 'oper.an' in file: \n",
    "            times = a.time.values.astype('datetime64[s]')\n",
    "            assert len(times) == 4\n",
    "            assert v.shape[0] == len(times)\n",
    "        elif 'oper.fc' in file:\n",
    "            start_times = np.repeat(a.forecast_initial_time.values.astype('datetime64[s]'), 2)\n",
    "            deltas = np.tile([np.timedelta64(int(h), 'h') for h in a.forecast_hour.values], 2)\n",
    "            times = list(zip(start_times, deltas))\n",
    "            # print(times)\n",
    "            v = v.reshape(4, v.shape[-2], v.shape[-1])\n",
    "            # print(times); print(deltas)\n",
    "        assert v.shape[1:] == lat.shape\n",
    "        assert v.shape[1:] == lon.shape\n",
    "        \n",
    "        \n",
    "    \n",
    "        zones = {}# defaultdict(dict)\n",
    "        \n",
    "        for tidx, t in enumerate(times):\n",
    "            for grid_id, plon, plat in coords[location]:\n",
    "                for r in [ 0.05, 0.1, 0.2, 0.5, 1, 2, 5]:\n",
    "                    if (grid_id, r) not in zones:\n",
    "                        zones[(grid_id, r)] = (lat - plat) ** 2 + (lon - plon%360) ** 2 < r ** 2\n",
    "                    zone = zones[(grid_id, r)]\n",
    "                    # ct = len(v[tidx][zone])#.count()\n",
    "                    sat_data[t][grid_id][clean_name + '_mean{}'.format(r)] = v[tidx][zone].mean() #if ct > 3 else np.nan\n",
    "    \n",
    "    # for k, v in sat_data.items():\n",
    "    #     print(k, len(v))\n",
    "    \n",
    "    # print(v['1X116']) \n",
    "    \n",
    "    def clean(d):\n",
    "        if isinstance(d, defaultdict):\n",
    "            d = {k: clean(v) for k, v in d.items()}\n",
    "        return d\n",
    "    \n",
    "    return clean(sat_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullIFS(files):\n",
    "    results=[]\n",
    "    \n",
    "    save_dir = '/tmp/'\n",
    "    processed_dir = 'inference/ifs/'  # Directory where processed files are stored\n",
    "    dspath = 'https://data.rda.ucar.edu/ds113.1/'\n",
    "    print('Checking and downloading {} ifs files'.format(len(files)))\n",
    "    zc = zstd.ZstdCompressor(level=9)\n",
    "    for file in files:\n",
    "        start = time.time()\n",
    "        filename = os.path.basename(file)\n",
    "        outfile = os.path.join(save_dir, filename)\n",
    "        processed_file = os.path.join(processed_dir, filename)\n",
    "\n",
    "        # Check if the file has already been processed\n",
    "        if os.path.exists(processed_file):\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "        try:\n",
    "            full_url = dspath + file\n",
    "            print('Downloading', file)\n",
    "            with requests.get(full_url, allow_redirects=True, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(outfile, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            s = os.path.getsize(outfile)\n",
    "            data = processIFS(outfile)  # Ensure this function is defined elsewhere\n",
    "            os.remove(outfile)\n",
    "            pkl = pickle.dumps(data)\n",
    "            compr = zc.compress(pkl)\n",
    "            os.makedirs('inference/ifs/', exist_ok=True)\n",
    "            with open('inference/ifs/{}'.format(os.path.basename(filename)), 'wb') as f:\n",
    "                f.write(compr)\n",
    "            results.append({\n",
    "                'file': os.path.basename(filename),\n",
    "                'body': s/1e6,\n",
    "                'outlen': len(pkl),\n",
    "                'outlen-compr': len(compr),\n",
    "                'elaspsed_time': round(time.time() - start, 1)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            try: os.remove(outfile)\n",
    "            except: pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listIFSFiles(dates):\n",
    "    filelist = []\n",
    "#   https://data.rda.ucar.edu/ds113.1/ec.oper.fc.sfc/201601/ec.oper.fc.sfc.128_031_ci.regn1280sc.20160101.nc\n",
    "    for t in dates:\n",
    "        for tag in ifs_tags:\n",
    "            domain = 'ec.oper.fc.sfc'\n",
    "            file =  '{}/{}/{}.{}.regn1280sc.{}.nc'.format(domain,\n",
    "                            datetime.datetime.strftime(t, '%Y%m'), \n",
    "                            domain, tag, \n",
    "                            datetime.datetime.strftime(t, '%Y%m%d') )\n",
    "            filelist.append(file)\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAssim(field, location, year, month, min_day, max_day):\n",
    "    url = 'https://opendap.nccs.nasa.gov/dods/gmao/geos-cf/assim/aqc_tavg_1hr_g1440x721_v1'\n",
    "    DATASET = xr.open_dataset(url)\n",
    "    start_time = np.datetime64('{}-{:02d}-{:02d} 00:00:00'.format(year, month, min_day))\n",
    "    end_time = np.datetime64('{}-{:02d}-{:02d} 23:59:00'.format(year, month, max_day))\n",
    "    # end_time = np.datetime64('{}-01-01 00:00:00'.format(year + 1))\n",
    "    minimum_latitude = min([e[-1] for e in coords[location]]) - 3\n",
    "    minimum_longitude = min([e[-2] for e in coords[location]]) - 3\n",
    "    maximum_latitude = max([e[-1] for e in coords[location]]) + 3\n",
    "    maximum_longitude = max([e[-2] for e in coords[location]]) + 3\n",
    "    data = DATASET[field].loc[{'time':slice(start_time,end_time),\n",
    "                                       'lat':slice(minimum_latitude,maximum_latitude),\n",
    "                                       'lon':slice(minimum_longitude,maximum_longitude)}]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAssim(a, location, field):\n",
    "    t = a.time.values.astype('datetime64[s]')\n",
    "    sat_data = defaultdict(dict)\n",
    "    v = a.values[0]\n",
    "    if (v == 1.0e15).sum() > 0:\n",
    "        return {'location': location, 'time_end': t, 'd1': cleanDict(sat_data)}\n",
    " \n",
    "    lat = np.tile( np.stack([a['lat']], axis = 1), ( 1, v.shape[1]))\n",
    "    lon = np.tile( np.stack([a['lon']], axis = 0), ( v.shape[0], 1))\n",
    "    lat = cv2.resize(lat, None, fx = 5, fy = 5)\n",
    "    lon = cv2.resize(lon, None, fx = 5, fy = 5)\n",
    "    v2 = cv2.resize(v, None, fx = 5, fy = 5)\n",
    "    zones = {}\n",
    "    for grid_id, plon, plat in coords[location]:\n",
    "        for r in [ 0.1, 0.25, 0.5, 1, 2, ]:\n",
    "            if (grid_id, r) not in zones:\n",
    "                z = (lat - plat) ** 2 + (lon - plon) ** 2 < r ** 2\n",
    "                zones[(grid_id, r)] = z#, z.sum())\n",
    "            zone = zones[(grid_id, r)]\n",
    "            m = v2[zone].mean()#, 1#zone.sum()\n",
    "            sat_data[grid_id][field + '_mean{}'.format(r)] = m #data[zone].mean()# if ct > 3 else np.nan\n",
    "    return {'location': location, 'time_end': t, 'd1': cleanDict(sat_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssim(year, month, min_day, max_day):\n",
    "    for field in ['no2', 'so2', 'co', 'o3', 'pm25_rh35_gcc']:\n",
    "        for location in coords.keys():\n",
    "            start = time.time()\n",
    "            filename = '{}_{}_{}_{:02}.pkl'.format(field, location, year, month)\n",
    "            file_path = 'inference/assim/{}'.format(filename)\n",
    "            \n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                print(\"File already exists. Skipping:\", file_path)\n",
    "                continue\n",
    "            \n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    data = loadAssim(field, location, year, month, min_day, max_day)\n",
    "                    print('{}-{:02d} {} {} {}'.format(year, month, field, location, len(data)))\n",
    "                    with parallel_backend('threading'):\n",
    "                        r = Parallel(os.cpu_count())(\n",
    "                            delayed(processAssim)(a, location, field) for a in data) \n",
    "                    zc = zstd.ZstdCompressor(level=9)\n",
    "                    out = pickle.dumps(r)\n",
    "                    compr = zc.compress(out)\n",
    "                    os.makedirs('inference/assim/', exist_ok=True)\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(compr)\n",
    "                    print({\n",
    "                        'file': filename.split('.')[0],\n",
    "                        'outlen': len(out),\n",
    "                        'outlen-compr': len(compr),\n",
    "                        'elapsed_time': round(time.time() - start, 1)\n",
    "                    }); break\n",
    "                except Exception as e:\n",
    "                    print(e); time.sleep(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listAssimDates(dates):\n",
    "    months = {}\n",
    "    for t in dates:\n",
    "        k = (t.year, t.month)\n",
    "        prior = months.get(k, [])\n",
    "        if sum(prior) > 0:\n",
    "            months[k] = (min(prior[0], t.day), max(prior[0], t.day))\n",
    "        else:\n",
    "            months[k] = (t.day, t.day)\n",
    "    return [(*k, *v) for k, v in months.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start & End Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108\n",
      "2018-08-22 00:00:00\n",
      "2021-09-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime(*[int(i) for i in infer['start'].split(',')])\n",
    "end = datetime.datetime(*[int(i) for i in infer['end'].split(',')])\n",
    "dt = start - datetime.timedelta(days = 10)\n",
    "dates = []\n",
    "while dt <= end + datetime.timedelta(days = 1):\n",
    "    dates.append(dt);\n",
    "    dt += datetime.timedelta(days = 1)\n",
    "print(len(dates))\n",
    "print(dates[0]); print(dates[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TROPOMI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropomi_fields = ['nitrogendioxide_tropospheric_column',\n",
    " 'nitrogendioxide_tropospheric_column_precision',\n",
    " 'air_mass_factor_troposphere',\n",
    " 'air_mass_factor_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTropomi(row):\n",
    "    # Inference File Checker\n",
    "    filename, url, sz = [row[k] for k in ['granule_id', 'us_url', 'granuleSize']]\n",
    "\n",
    "    inference_file_dir = 'inference/tropomi-fine/'\n",
    "    os.makedirs(inference_file_dir, exist_ok = True)\n",
    "\n",
    "    infer_file = os.path.join(inference_file_dir, filename)\n",
    "    print(f'infer filename is: {infer_file}')\n",
    "    if os.path.exists(infer_file):\n",
    "        print(f\"{infer_file} already exists. Skipping download.\")\n",
    "        return infer_file, False\n",
    "    for i in range(1):\n",
    "        ret = None\n",
    "        try:\n",
    "            values = {'email' : secure['username'], 'passwd' : secure['password'], 'action' : 'login'}\n",
    "            login_url = 'https://urs.earthdata.nasa.gov'\n",
    "            ret = requests.post(login_url, data=values)\n",
    "        \n",
    "            if ret.status_code == 200:\n",
    "                print(\"Login successful.\")\n",
    "            else:\n",
    "                print(\"Bad Authentication\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(i)\n",
    "\n",
    "    zc = zstd.ZstdCompressor(level=15)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    #print(filename, url, sz) \n",
    "    tmp_file = '/tmp/' + filename\n",
    "    try:\n",
    "        print(\"Downloading\", filename)\n",
    "        if os.path.exists(tmp_file):\n",
    "            print(f\"{tmp_file} already exists. Skipping download.\")\n",
    "            pass\n",
    "        with requests.get(url, cookies = ret.cookies, \n",
    "            allow_redirects = True, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(tmp_file, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024): \n",
    "                        f.write(chunk)\n",
    "        print(f\"Downloaded and compressed {filename} to {tmp_file}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    " \n",
    "    return tmp_file, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTropomi(hdf, fine = True):\n",
    "    zones = {}; # defaultdict(dict)\n",
    "    sat_data = defaultdict(lambda: defaultdict(dict))\n",
    "    hp = hdf['PRODUCT']\n",
    "    lat = hp['latitude'][:][0]#.values\n",
    "    lon = hp['longitude'][:][0]#.values\n",
    "    \n",
    "    for field in tropomi_fields:\n",
    "        v = hp[field][:][0]\n",
    "        data = np.ma.masked_array(v, (v == v.max() ) | (v == v.min())).clip(0, None)\n",
    "        assert data.shape == lat.shape\n",
    "        \n",
    "        for grid_id, plon, plat in coords['la']:\n",
    "            for r in  ([ 0.07, 0.1, 0.14, 0.2, 0.3, 0.5, 1, 2] if fine else [ 0.1, 0.25, 0.5, 1, 2, ]):\n",
    "                if (grid_id, r) not in zones:\n",
    "                    zones[(grid_id, r)] = (lat - plat) ** 2 + (lon - plon) ** 2 < r ** 2\n",
    "                zone = zones[(grid_id, r)]\n",
    "                ct = data[zone].count()\n",
    "                m = data[zone].mean() if ct > (0 if 'fine' else 3) else np.nan\n",
    "                s = data[zone].std() if ct >= 3 else np.nan\n",
    "                sat_data[grid_id][field + '_mean{}'.format(r)] = m\n",
    "                sat_data[grid_id][field + '_stdev{}'.format(r)] = s\n",
    "                sat_data[grid_id][field + '_count{}'.format(r)] = ct\n",
    "                # if '2' in grid_id:#.startswith('9'):\n",
    "                    # print(field, '_count{}'.format(r), ct, m ,s )\n",
    "    return sat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullTropomi(row, fine = True):\n",
    "    sat_data=None\n",
    "    start = time.time()\n",
    "    #assert row['product'].startswith('tropomi')\n",
    "    \n",
    "    tropomi_file, downloaded = loadTropomi(row)\n",
    "    if not downloaded:\n",
    "        print(f\"Skipping further processing for {tropomi_file} as it already exists.\")\n",
    "        return\n",
    "    with h5py.File(tropomi_file, 'r') as hdf:\n",
    "        sat_data = processTropomi(hdf, fine)\n",
    "    s = os.path.getsize(tropomi_file); \n",
    "    #print(sat_data)\n",
    "    output = row.copy()\n",
    "    output['d1'] = cleanDict(sat_data)\n",
    "    \n",
    "    zc = zstd.ZstdCompressor(level = 15)\n",
    "    pkl = cloudpickle.dumps(output)\n",
    "    compr = zc.compress(pkl)\n",
    "    \n",
    "    filename = tropomi_file.split('/')[-1]\n",
    "    os.makedirs('inference/tropomi-fine/', exist_ok = True)\n",
    "    with open('inference/tropomi-fine/{}'.format(filename), 'wb') as f:\n",
    "        f.write(compr)\n",
    "        \n",
    "    try:     \n",
    "        os.remove(tropomi_file) \n",
    "        print(f'{tropomi_file} Successfuly Deleted')\n",
    "    except:\n",
    "        print(f'{tropomi_file} Failed to Delete')\n",
    "        pass\n",
    "    return {\n",
    "        # 'statusCode': 200,\n",
    "        'file': os.path.basename(filename),\n",
    "        'body': s/1e6, #os.path.getsize(outfile), #json.dumps('Hello from Lambda!'),\n",
    "        'outlen': len(pkl),#len(pickle.dumps(data)),\n",
    "        'outlen-compr': len(compr),#zc.compress(pickle.dumps(data))),\n",
    "        'elaspsed_time': round(time.time() - start, 1)\n",
    "    }; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listTropomiRows(dates):\n",
    "    tropomi_rows = [e.to_dict() for idx, e in \n",
    "         files[files['product'].str.startswith('tropomi')\n",
    "               & (files.time_end.dt.tz_localize(None) >= min(dates) )\n",
    "               & (files.time_end.dt.tz_localize(None) \n",
    "                      <= max(dates) + datetime.timedelta(days = 1) )\n",
    "              ].iterrows()]\n",
    "    return tropomi_rows\n",
    "    # print(tropomi_rows)\n",
    "# listTropomiRows(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_files = pd.read_csv('data_tg/tempo_no2_metadata.csv')\n",
    "tempo_files.time_end = pd.to_datetime(tempo_files.time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_fields = ['vertical_column_total',\n",
    "'vertical_column_total_uncertainty',\n",
    "'main_data_quality_flag',\n",
    "'vertical_column_troposphere',\n",
    "'vertical_column_stratosphere',\n",
    "'vertical_column_troposphere_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTempo(row):\n",
    "    # Inference File Checker\n",
    "    filename, url, sz = [row[k] for k in ['granule_id', 'us_url', 'granuleSize']]\n",
    "\n",
    "    inference_file_dir = 'inference/tempo-fine/'\n",
    "    os.makedirs(inference_file_dir, exist_ok = True)\n",
    "\n",
    "    infer_file = os.path.join(inference_file_dir, filename)\n",
    "    # print(f'infer filename is: {infer_file}')\n",
    "    if os.path.exists(infer_file):\n",
    "        print(f\"{infer_file} already exists. Skipping download.\")\n",
    "        return infer_file, False\n",
    "    for i in range(1):\n",
    "        ret = None\n",
    "        try:\n",
    "            values = {'email' : secure['username'], 'passwd' : secure['password'], 'action' : 'login'}\n",
    "            login_url = 'https://urs.earthdata.nasa.gov'\n",
    "            ret = requests.post(login_url, data=values)\n",
    "        \n",
    "            if ret.status_code == 200:\n",
    "                print(\"Login successful.\")\n",
    "            else:\n",
    "                print(\"Bad Authentication\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(i)\n",
    "\n",
    "    zc = zstd.ZstdCompressor(level=15)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    #print(filename, url, sz) \n",
    "    tmp_file = '/tmp/' + filename\n",
    "    try:\n",
    "        print(\"Downloading\", filename)\n",
    "        if os.path.exists(tmp_file):\n",
    "            print(f\"{tmp_file} already exists. Skipping download.\")\n",
    "            pass\n",
    "        with requests.get(url, cookies = ret.cookies, \n",
    "            allow_redirects = True, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(tmp_file, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024*1024): \n",
    "                        f.write(chunk)\n",
    "        print(f\"Downloaded and compressed {filename} to {tmp_file}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    " \n",
    "    return tmp_file, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTempo(hdf, fine = True):\n",
    "    zones = {}; # defaultdict(dict)\n",
    "    sat_data = defaultdict(lambda: defaultdict(dict))\n",
    "    hp = hdf['product']\n",
    "    lat = hdf['geolocation']['latitude'][:][0]#.values\n",
    "    lon = hdf['geolocation']['longitude'][:][0]#.values\n",
    "    \n",
    "    for field in tempo_fields:\n",
    "        v = hp[field][:][0]\n",
    "        data = np.ma.masked_array(v, (v == v.max() ) | (v == v.min())).clip(0, None)\n",
    "        assert data.shape == lat.shape\n",
    "        \n",
    "        for grid_id, plon, plat in coords['la']:\n",
    "            for r in  ([ 0.07, 0.1, 0.14, 0.2, 0.3, 0.5, 1, 2] if fine else [ 0.1, 0.25, 0.5, 1, 2, ]):\n",
    "                if (grid_id, r) not in zones:\n",
    "                    zones[(grid_id, r)] = (lat - plat) ** 2 + (lon - plon) ** 2 < r ** 2\n",
    "                zone = zones[(grid_id, r)]\n",
    "                ct = data[zone].count()\n",
    "                m = data[zone].mean() if ct > (0 if 'fine' else 3) else np.nan\n",
    "                s = data[zone].std() if ct >= 3 else np.nan\n",
    "\n",
    "                sat_data[grid_id][field + '_mean{}'.format(r)] = m\n",
    "                sat_data[grid_id][field + '_stdev{}'.format(r)] = s\n",
    "                sat_data[grid_id][field + '_count{}'.format(r)] = ct\n",
    "                # if '2' in grid_id:#.startswith('9'):\n",
    "                    # print(field, '_count{}'.format(r), ct, m ,s )\n",
    "    return sat_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullTempo(row, fine = True):\n",
    "    sat_data=None\n",
    "    start = time.time()\n",
    "    #assert row['product'].startswith('tempo')\n",
    "    \n",
    "    tempo_file, downloaded = loadTempo(row)\n",
    "    if not downloaded:\n",
    "        print(f\"Skipping further processing for {tempo_file} as it already exists.\")\n",
    "        return\n",
    "    with h5py.File(tempo_file, 'r') as hdf:\n",
    "        sat_data = processTempo(hdf, fine)\n",
    "    s = os.path.getsize(tempo_file); \n",
    "    #print(sat_data)\n",
    "    output = row.copy()\n",
    "    output['d1'] = cleanDict(sat_data)\n",
    "    \n",
    "    zc = zstd.ZstdCompressor(level = 15)\n",
    "    pkl = cloudpickle.dumps(output)\n",
    "    compr = zc.compress(pkl)\n",
    "    \n",
    "    filename = tempo_file.split('/')[-1]\n",
    "    os.makedirs('inference/tempo-fine/', exist_ok = True)\n",
    "    with open('inference/tempo-fine/{}'.format(filename), 'wb') as f:\n",
    "        f.write(compr)\n",
    "        \n",
    "    try:     \n",
    "        os.remove(tempo_file) \n",
    "        print(f'{tempo_file} Successfuly Deleted')\n",
    "    except:\n",
    "        print(f'{tempo_file} Failed to Delete')\n",
    "        pass\n",
    "    return {\n",
    "        # 'statusCode': 200,\n",
    "        'file': os.path.basename(filename),\n",
    "        'body': s/1e6, #os.path.getsize(outfile), #json.dumps('Hello from Lambda!'),\n",
    "        'outlen': len(pkl),#len(pickle.dumps(data)),\n",
    "        'outlen-compr': len(compr),#zc.compress(pickle.dumps(data))),\n",
    "        'elaspsed_time': round(time.time() - start, 1)\n",
    "    }; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all rows & columns in tempo_files\n",
    "def listTempoRows():\n",
    "    tempo_rows = [e.to_dict() for idx, e in tempo_files[\n",
    "        tempo_files['product'].str.startswith('tempo')\n",
    "    ].iterrows()]\n",
    "    return tempo_rows\n",
    "#     print(tempo_rows)\n",
    "\n",
    "# listTempoRows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Parallel Process Tempo Data\n",
    "if __name__ == '__main__':\n",
    "    N_THREADS = min(5, os.cpu_count())\n",
    "    Parallel(N_THREADS)(delayed(pullTempo)(row)\n",
    "                        for row in listTempoRows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Parallel Process Tropomi Data\n",
    "if __name__ == '__main__':\n",
    "    N_THREADS = min(5, os.cpu_count())\n",
    "    Parallel(N_THREADS)(delayed(pullTropomi)(row)\n",
    "                        for row in listTropomiRows(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Process IFS Data\n",
    "if __name__ == '__main__':\n",
    "    N_THREADS = min(10, os.cpu_count() )\n",
    "    Parallel(N_THREADS)(delayed(pullIFS)(\n",
    "        listIFSFiles(dates)[i::N_THREADS])\n",
    "                            for i in range(N_THREADS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Process GFS Data\n",
    "if __name__ == '__main__':\n",
    "    N_THREADS = min(4, os.cpu_count() )\n",
    "    Parallel(N_THREADS)(delayed(pullGFS)(\n",
    "       listGFSFiles(dates)[:][i::N_THREADS]) \n",
    "                           for i in range(N_THREADS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping: inference/assim/no2_la_2018_08.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2018_08.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2018_08.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2018_08.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2018_08.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2018_09.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2018_09.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2018_09.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2018_09.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2018_09.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2018_10.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2018_10.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2018_10.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2018_10.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2018_10.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2018_11.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2018_11.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2018_11.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2018_11.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2018_11.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2018_12.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2018_12.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2018_12.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2018_12.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2018_12.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_01.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_01.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_01.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_01.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_01.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_02.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_02.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_02.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_02.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_02.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_03.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_03.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_03.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_03.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_03.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_04.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_04.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_04.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_04.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_04.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_05.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_05.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_05.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_05.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_05.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_06.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_06.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_06.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_06.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_06.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_07.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_07.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_07.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_07.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_07.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_08.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_08.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_08.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_08.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_08.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_09.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_09.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_09.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_09.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_09.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_10.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_10.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_10.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_10.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_10.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_11.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_11.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_11.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_11.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_11.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2019_12.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2019_12.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2019_12.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2019_12.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2019_12.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_01.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_01.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_01.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_01.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_01.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_02.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_02.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_02.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_02.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_02.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_03.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_03.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_03.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_03.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_03.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_04.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_04.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_04.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_04.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_04.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_05.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_05.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_05.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_05.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_05.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_06.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_06.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_06.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_06.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_06.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_07.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_07.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_07.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_07.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_07.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_08.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_08.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_08.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_08.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_08.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_09.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_09.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_09.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_09.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_09.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_10.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_10.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_10.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_10.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_10.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_11.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_11.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_11.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_11.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_11.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2020_12.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2020_12.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2020_12.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2020_12.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2020_12.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_01.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_01.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_01.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_01.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_01.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_02.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_02.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_02.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_02.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_02.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_03.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_03.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_03.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_03.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_03.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_04.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_04.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_04.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_04.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_04.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_05.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_05.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_05.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_05.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_05.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_06.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_06.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_06.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_06.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_06.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_07.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_07.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_07.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_07.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_07.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_08.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_08.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_08.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_08.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_08.pkl\n",
      "File already exists. Skipping: inference/assim/no2_la_2021_09.pkl\n",
      "File already exists. Skipping: inference/assim/so2_la_2021_09.pkl\n",
      "File already exists. Skipping: inference/assim/co_la_2021_09.pkl\n",
      "File already exists. Skipping: inference/assim/o3_la_2021_09.pkl\n",
      "File already exists. Skipping: inference/assim/pm25_rh35_gcc_la_2021_09.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    [pullAssim(*d) for d in listAssimDates(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tempo without parallel processing\n",
    "# def tempo_run():\n",
    "#     for row in listTempoRows():\n",
    "#         pullTempo(row)\n",
    "# if __name__ == '__main__':\n",
    "#     tempo_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TROPOMI without parallel processing\n",
    "# def tropomi_run():\n",
    "#     for row in listTropomiRows(dates):\n",
    "#         pullTropomi(row)\n",
    "# if __name__ == '__main__':\n",
    "#     tropomi_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start.year <= 2018 and end.year >= 2021:\n",
    "    os.makedirs('cache', exist_ok = True)\n",
    "    for path in os.listdir('inference'):\n",
    "        with tarfile.open('cache/{}.tar'.format(path), 'w') as f:\n",
    "            for file in os.listdir('inference/{}'.format(path)):\n",
    "                f.add('inference/{}/{}'.format(path, file), \n",
    "                          arcname = file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jupyter nbconvert --no-prompt --to script 'RunFeatures.ipynb' <br>\n",
    "if __name__ == __main__:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
