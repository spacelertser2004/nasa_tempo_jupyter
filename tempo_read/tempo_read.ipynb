{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattl\\AppData\\Local\\Temp\\ipykernel_8836\\2865044542.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import time\n",
    "import csv\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_dir = 'C:/aiarthon_py/nasa-airathon-merge/secure.txt'\n",
    "secure = dict([e.split('=') for e in open(scr_dir, 'r').read().split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_row_dir = 'C:/aiarthon_py/nasa-airathon-merge/tempo_read/tempo_rows.txt'\n",
    "rows = [line.strip() for line in open(tempo_row_dir, 'r') if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_var_and_wr_csv(file_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Process NetCDF files in a given directory to extract metadata and write to a CSV file.\n",
    "    Parameters:\n",
    "    - file_dir: Directory containing the NetCDF files.\n",
    "    - output_csv_path: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "        \n",
    "    # List all NetCDF files in the specified directory\n",
    "    files = [f for f in os.listdir(file_dir) if f.endswith('.nc')]\n",
    "\n",
    "    # Define headers for the CSV file based on the metadata to extract\n",
    "    headers = ['granule_id', 'time_start', 'time_end', 'product', 'location', 'split', 'granuleSize']\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_csv_path, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        if os.stat(output_csv_path).st_size == 0:\n",
    "            csvwriter.writeheader()  # Write header only if file is empty\n",
    "        \n",
    "        # Process each NetCDF file\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(file_dir, file_name)\n",
    "            with Dataset(file_path, 'r') as nc:\n",
    "                # Extract metadata; adjust these as necessary to match your NetCDF structure\n",
    "                timeStart = getattr(nc, 'time_coverage_start', 'NA')\n",
    "                timeEnd = getattr(nc, 'time_coverage_end', 'NA')\n",
    "                product = 'tempo'\n",
    "                location = 'la'\n",
    "                split = 'train'\n",
    "                granuleSize = os.path.getsize(file_path)\n",
    "                # Write the extracted metadata to the CSV\n",
    "                csvwriter.writerow({\n",
    "                    'granule_id': file_name,\n",
    "                    'time_start': timeStart,\n",
    "                    'time_end': timeEnd,\n",
    "                    'product': product,\n",
    "                    'location': location,\n",
    "                    'split' : split,\n",
    "                    'granuleSize': granuleSize,\n",
    "                })\n",
    "                \n",
    "            # Delete the file after processing\n",
    "            print(f\"Successfuly Written: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def os_remove():\n",
    "    tmp_dir = '/home/sagemaker-user/tempo_data/tmp/'  # The directory from which you want to delete files\n",
    "    files = [f for f in os.listdir(tmp_dir) if f.endswith('.nc')]  # List of all .nc files in the directory\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = os.path.join(tmp_dir, filename)  # Full path to the file\n",
    "        try:\n",
    "            os.remove(file_path)  # Delete the file\n",
    "            print(f\"Successfuly Deleted: {filename} from {tmp_dir}\")\n",
    "        except FileNotFoundError:  # Catch the specific exception if the file does not exist\n",
    "            print(f\"{filename} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFileS3(rows):\n",
    "    for i in range(1):\n",
    "        try:\n",
    "            values = {'email' : secure['username'], 'passwd' : secure['password'], 'action' : 'login'}\n",
    "            login_row = 'https://urs.earthdata.nasa.gov'\n",
    "            ret = requests.post(login_row, data=values)\n",
    "            if ret.status_code == 200:\n",
    "                print(\"Login successful.\")\n",
    "            else:\n",
    "                print(\"Bad Authentication\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(i)\n",
    "        \n",
    "    # zc = zstd.ZstdCompressor(level=15)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    for row in rows:\n",
    "        try:\n",
    "            outfile = os.path.basename(row)\n",
    "            print(\"Downloading\", outfile)\n",
    "            with requests.get(row, cookies = ret.cookies, \n",
    "                  allow_redirects = True, stream=True) as r:\n",
    "                  r.raise_for_status()\n",
    "                  outfile_path = os.path.join(download_dir, outfile)\n",
    "                  with open(outfile_path, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=1024*1024): \n",
    "                              f.write(chunk)\n",
    "                  \n",
    "\n",
    "            # Extract filename from row and append '.zst' for the compressed version\n",
    "            filename = rows.split('/')[-1]\n",
    "            save_path = os.path.join(download_dir, filename)\n",
    "            print(f\"Downloaded and compressed {filename} to {save_path}\")\n",
    "            print(\"Extracting variables and writing to CSV.\")\n",
    "            extract_var_and_wr_csv('/home/sagemaker-user/tempo_data/tmp/', '/csv/output.csv')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading {row}: {e}\")\n",
    "        finally:\n",
    "            print(f\"Deleting: {filename}\")\n",
    "            os_remove()\n",
    "        \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filename' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mloadFileS3\u001b[1;34m(rows)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     outfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[43murl\u001b[49m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m, outfile)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloadFileS3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#os_remove()\u001b[39;00m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mloadFileS3\u001b[1;34m(rows)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m         os_remove()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_path\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'filename' referenced before assignment"
     ]
    }
   ],
   "source": [
    "loadFileS3(rows)\n",
    "#os_remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
