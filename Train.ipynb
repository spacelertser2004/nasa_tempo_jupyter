{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt<br>\n",
    "!pip install pipreqs<br>\n",
    "print(os.listdir('data_tg'))<br>\n",
    "!pipreqs --force<br>\n",
    "!jupyter nbconvert --to script 'Train.ipynb'<br>\n",
    "!pip install netCDF4<br>\n",
    "!pip install h5py<br>\n",
    "!pip install pyhdf<br>\n",
    "!pip install basemap<br>\n",
    "!pip install pydap<br>\n",
    "!pip install xarray<br>\n",
    "!pip install pygrib<br>\n",
    "!pip install opencv-python<br>\n",
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import sklearn\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import xarray as xr\n",
    "import pydap\n",
    "import h5py\n",
    "from pyhdf.SD import SD, SDC\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib as mpl\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import secrets\n",
    "import datetime\n",
    "import pygrib\n",
    "import time\n",
    "import zstandard as zstd\n",
    "import tarfile\n",
    "import io\n",
    "import requests\n",
    "import pickle\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "get_ipython().system('pip install opencv-python-headless')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.max_open_warning'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3 = boto3.client('s3')<br>\n",
    "sqs = boto3.client('sqs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zd = zstd.ZstdDecompressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = datetime.datetime.now().microsecond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)\n",
    "run_label = secrets.token_hex(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSIM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data_{}/train_labels.csv'.format(dataset))\n",
    "grid = pd.concat((\n",
    "    pd.read_csv('data_tg/grid_metadata.csv'),\n",
    "    # pd.read_csv('data_pm/grid_metadata.csv')\n",
    ")).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data_{}/submission_format.csv'.format(dataset))\n",
    "files = pd.read_csv('data_{}/{}_satellite_metadata{}.csv'.format(\n",
    "                    dataset, *(('pm25', '') if dataset == 'pm'\n",
    "                               else ('no2', '_0AF3h09'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['location'] = grid.set_index(\n",
    "    'grid_id')['location'].reindex(labels.grid_id).values\n",
    "labels['datetime'] = pd.to_datetime(labels.datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['location'] = grid.set_index(\n",
    "    'grid_id').location.reindex(submission.grid_id).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc_dict = {'la': 'Los Angeles (SoCAB)', 'tpe': 'Taipei', 'dl': 'Delhi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dict = {'la': 'Los Angeles (SoCAB)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = defaultdict(list)\n",
    "for city, gcity in loc_dict.items():  # ['Delhi', 'Taipei', 'LA']:\n",
    "    # gcity = [c for c in grid.location.unique() if 'Los A' in c][0] if city not in grid.location.unique() else city\n",
    "    grid_points = grid[grid.location == gcity]\n",
    "    for e in grid_points.itertuples():\n",
    "        coords[city].append((e.grid_id, *np.array([(float(p.split(' ')[0]),\n",
    "                                                    float(p.split(' ')[1]))\n",
    "                                                   for p in e.wkt[10:].split(', ')[:4]]).mean(axis=0).round(4).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDict(d):\n",
    "    return {\n",
    "        k: cleanDict(v) for k,\n",
    "        v in d.items()} if isinstance(\n",
    "        d,\n",
    "        defaultdict) else d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([len(v) for c, v in coords.items()]) == grid.grid_id.nunique()\n",
    "grid.grid_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = cleanDict(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = datetime.datetime(2016, 1, 1)<br>\n",
    "# t = datetime.datetime(2020, 5, 26)<br>\n",
    "while t < datetime.datetime.now() - datetime.timedelta(days = 1):<br>\n",
    "    print(t)<br>\n",
    "    for tag in ifs_tags:<br>\n",
    "    # for tag in ifs_forecasts:# ifs_tags:#[:1]:<br>\n",
    "        domain = 'ec.oper.an.sfc'<br>\n",
    "        # domain = 'ec.oper.fc.sfc'<br>\n",
    "        file =  '{}/{}/{}.{}.regn1280sc.{}.nc'.format(domain,<br>\n",
    "                        datetime.datetime.strftime(t, '%Y%m'),<br>\n",
    "                        domain, tag,<br>\n",
    "                        datetime.datetime.strftime(t, '%Y%m%d') )<br>\n",
    "        if file.split('/')[-1] not in existing_files:<br>\n",
    "            sqs.send_message(<br>\n",
    "                QueueUrl='https://sqs.us-east-2.amazonaws.com/815054066888/aqi-ifs',<br>\n",
    "                MessageBody=json.dumps({'file': file}<br>\n",
    "            ) )<br>\n",
    "        # else:<br>\n",
    "        #     print(file, 'exists')<br>\n",
    "    t += datetime.timedelta(days = 1)<br>\n",
    "    # break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['nitrogendioxide_tropospheric_column',\n",
    "          'nitrogendioxide_tropospheric_column_precision',\n",
    "          'air_mass_factor_troposphere',\n",
    "          'air_mass_factor_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trop_scr_dir = 'tropomi_data/secure.txt'\n",
    "trop_secure = dict([e.split('=')\n",
    "                   for e in open(trop_scr_dir, 'r').read().split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(row):\n",
    "    for i in range(1):\n",
    "        try:\n",
    "            values = {\n",
    "                'email': trop_secure['username'],\n",
    "                'passwd': trop_secure['password'],\n",
    "                'action': 'login'}\n",
    "            login_url = 'https://urs.earthdata.nasa.gov'\n",
    "            ret = requests.post(login_url, data=values)\n",
    "            if ret.status_code == 200:\n",
    "                print(\"Login successful.\")\n",
    "            else:\n",
    "                print(\"Bad Authentication\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(i)\n",
    "    zc = zstd.ZstdCompressor(level=15)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    filename, url, sz = [row[k]\n",
    "                         for k in ['granule_id', 'us_url', 'granuleSize']]\n",
    "    # print(filename, url, sz)\n",
    "    file = '/tmp/' + filename\n",
    "    try:\n",
    "        print(\"Downloading\", filename)\n",
    "        if os.path.exists(file):\n",
    "            print(f\"{file} already exists. Skipping download.\")\n",
    "            pass\n",
    "        with requests.get(url, cookies=ret.cookies,\n",
    "                          allow_redirects=True, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(file, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded and compressed {filename} to {file}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=sklearn.exceptions.ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=sklearn.exceptions.FitFailedWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['nitrogendioxide_tropospheric_column',\n",
    "          'nitrogendioxide_tropospheric_column_precision',\n",
    "          'air_mass_factor_troposphere',\n",
    "          'air_mass_factor_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hdf, location, fine=True):\n",
    "    zones = {}  # defaultdict(dict)\n",
    "    sat_data = defaultdict(lambda: defaultdict(dict))\n",
    "    hp = hdf['PRODUCT']\n",
    "    lat = hp['latitude'][:][0]  # .values\n",
    "    lon = hp['longitude'][:][0]  # .values\n",
    "    for field in fields:\n",
    "        v = hp[field][:][0]\n",
    "        data = np.ma.masked_array(\n",
    "            v, (v == v.max()) | (\n",
    "                v == v.min())).clip(\n",
    "            0, None)\n",
    "        assert data.shape == lat.shape\n",
    "        for grid_id, plon, plat in coords[location]:\n",
    "            for r in ([0.07, 0.1, 0.14, 0.2, 0.3, 0.5, 1, 2]\n",
    "                      if fine else [0.1, 0.25, 0.5, 1, 2, ]):\n",
    "                if (grid_id, r) not in zones:\n",
    "                    zones[(grid_id, r)] = (lat - plat) ** 2 + \\\n",
    "                        (lon - plon) ** 2 < r ** 2\n",
    "                zone = zones[(grid_id, r)]\n",
    "                ct = data[zone].count()\n",
    "                m = data[zone].mean() if ct > (0 if 'fine' else 3) else np.nan\n",
    "                s = data[zone].std() if ct >= 3 else np.nan\n",
    "                sat_data[grid_id][field + '_mean{}'.format(r)] = m\n",
    "                sat_data[grid_id][field + '_stdev{}'.format(r)] = s\n",
    "                sat_data[grid_id][field + '_count{}'.format(r)] = ct\n",
    "                if '2' in grid_id:  # .startswith('9'):\n",
    "                    print(field, '_count{}'.format(r), ct, m, s)\n",
    "    return sat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTropomi(row, fine=True):\n",
    "    assert row['product'].startswith('tropomi')\n",
    "\n",
    "    # Presumably, loadFile is a function you've defined elsewhere to load the\n",
    "    # data file\n",
    "    file = loadFile(row)\n",
    "    hdf = h5py.File(file, 'r')\n",
    "\n",
    "    # Assuming run is a function you've defined that processes the hdf file\n",
    "    # based on the location and 'fine' parameter\n",
    "    sat_data = run(hdf, row['location'], fine)\n",
    "    output = row.copy()\n",
    "\n",
    "    # Assuming cleanDict is a function you've defined to clean or process the\n",
    "    # satellite data in some way\n",
    "    output['d1'] = cleanDict(sat_data)\n",
    "\n",
    "    # Compress the serialized output\n",
    "    zc = zstd.ZstdCompressor(level=15)\n",
    "    o = pickle.dumps(output)\n",
    "    c = zc.compress(o)\n",
    "\n",
    "    # Save the compressed data locally\n",
    "    local_dir = \"path/to/your/local/directory\"  # Specify your directory here\n",
    "    filename = 'aqi_tropomi{}-{}.zst'.format(\n",
    "        '-fine' if fine else '', os.path.basename(file))\n",
    "    local_path = os.path.join(local_dir, filename)\n",
    "    with open(local_path, 'wb') as f:\n",
    "        f.write(c)\n",
    "    os.remove(file)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'len': len(o),\n",
    "        'clen': len(c),\n",
    "        'body': json.dumps('Success'),\n",
    "        'localPath': local_path  # Optionally return the path where the file was saved\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runTropomi(rows[-4], fine = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rows[-1]<br>\n",
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    '# Parallel(os.cpu_count() #// 2\\n#             )(delayed(runTropomi)(row) for row in rows)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = pd.to_datetime(files[(files['product'] == 'misr')<br>\n",
    "       & (files.location == 'la')].time_end)<br>\n",
    "d = d.sort_values().reset_index(drop = True)<br>\n",
    "(d.diff().dt.total_seconds() / (60*60*24)).iloc[:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coords['LA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqs.send_message(<br>\n",
    "    QueueUrl='https://sqs.us-east-2.amazonaws.com/815054066888/aqi-gfs25-extra',<br>\n",
    "    MessageBody=json.dumps((2020, 1, 1, 6, 0))<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = datetime.datetime(2016, 10, 1)<br>\n",
    "while t < datetime.datetime.now():<br>\n",
    "    print(t)<br>\n",
    "    for hr in range(0, 24, 6):<br>\n",
    "        for fwd in (0, ):<br>\n",
    "            sqs.send_message(<br>\n",
    "                QueueUrl='https://sqs.us-east-2.amazonaws.com/815054066888/aqi-gfs25-extra',<br>\n",
    "                MessageBody=json.dumps((t.year, t.month, t.day, hr, fwd))<br>\n",
    "            )<br>\n",
    "    t += datetime.timedelta(days = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[e[0] for e in pickle.loads(zd.decompress(<br>\n",
    "    s3.get_object(Bucket = 'projects-v',<br>\n",
    "              Key = 'aqi/gfs-5/{}'.format('gfs.0p25.2016100218.f000.grib2'))['Body'].read()<br>\n",
    ")) if 'LA' in e[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[e[0] for e in pickle.loads(<br>\n",
    "    s3.get_object(Bucket = 'projects-v',<br>\n",
    "              Key = 'aqi/gfs/{}'.format('gfs.0p25.2016100112.f000.grib2'))['Body'].read()<br>\n",
    ") if 'LA' in e[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[e for e in pickle.loads(<br>\n",
    "    s3.get_object(Bucket = 'projects-v',<br>\n",
    "              Key = 'aqi/gfs/{}'.format('gfs.0p25.2019100612.f000.grib2'))['Body'].read()<br>\n",
    ") if 'metre temp' in e[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid.wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid.sort_values('location').wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = pygrib.open(# 'gfs.0p25.2020072500.f000.grib2')#'gfs.0p25.2020010100.f000.grib2')<br>\n",
    "    # 'gfs.0p25.2017072500.f000.grib2'<br>\n",
    "    'gfs.0p25.2017072500.f000.grib2'<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cities = {<br>\n",
    " 'Taipei': ( (121.5, 121.5), (25.0, 25) ),<br>\n",
    " 'Delhi': ( (77.0, 77.25), (28.75, 28.5) ),<br>\n",
    " 'LA': ((360-118.25, 360-117.75), (34.0, 34.0) )<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {\n",
    "    'LA': ((360 - 118.25, 360 - 117.75), (34.0, 34.0))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', 'coords = defaultdict(list)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for city in ['Delhi', 'Taipei', 'LA']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['LA']:\n",
    "    gcity = [c for c in grid.location.unique(\n",
    "    ) if 'Los A' in c][0] if city not in grid.location.unique() else city\n",
    "    grid_points = grid[grid.location == gcity]\n",
    "    for e in grid_points.itertuples():\n",
    "        coords[city].append((e.grid_id, *np.array([(float(p.split(' ')[0]),\n",
    "                                                    float(p.split(' ')[1]))\n",
    "                                                   for p in e.wkt[10:].split(', ')[:4]]).mean(axis=0).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFiles(file):\n",
    "    zd = zstd.ZstdDecompressor()\n",
    "    data = []\n",
    "    with tarfile.TarFile(file, 'r') as tar:\n",
    "        assim_files = tar.getnames()\n",
    "        for file in assim_files:\n",
    "            if len(file) > 0:\n",
    "                f = tar.extractfile(file)\n",
    "                data.append(pickle.loads(zd.decompress(f.read())))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def listFiles(prefix, bucket = 'projects-v'):<br>\n",
    "    paginator = s3.get_paginator('list_objects_v2')<br>\n",
    "    page_iterator = paginator.paginate(Bucket = bucket,<br>\n",
    "                                       Prefix = prefix )<br>\n",
    "    files = []<br>\n",
    "    for page in page_iterator:<br>\n",
    "        files.extend([e['Key'] for e in page['Contents']])<br>\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def listFiles(prefix):<br>\n",
    "    return [os.path.join('cache/', prefix[4:], f)<br>\n",
    "            for f in os.listdir('cache/' + prefix[4:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loadData(file):<br>\n",
    "    zd = zstd.ZstdDecompressor()<br>\n",
    "    data = pickle.loads(zd.decompress(open(file, 'rb').read()))<br>\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l): return [e for s in l for e in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc_dict = {'la': 'Los Angeles (SoCAB)', 'tpe': 'Taipei', 'dl': 'Delhi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dict = {'la': 'Los Angeles (SoCAB)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rloc_dict = {v: k for k, v in loc_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_dict = grid.groupby('location').tz.first().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_tz_dict = {rloc_dict[k]: v for k, v in tz_dict.items()}\n",
    "loc_tz_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def downloadFile(file):<br>\n",
    "    s3 = boto3.client('s3')<br>\n",
    "    s3.download_file('projects-v', file, 'cache/' + file[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def downloadFiles(files):<br>\n",
    "    for path in set([f.split('/')[1] for f in files]):<br>\n",
    "        os.makedirs('cache/{}'.format(path), exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parallel(os.cpu_count() * 3)(delayed(downloadFile)(file)<br>\n",
    "                                     for file in files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloadFiles(assim_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time', '', \"all_data_assim = loadFiles('cache/assim.tar')\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_assim = flatten(all_data_assim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data_assim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(d, e):\n",
    "    t = d['time_end']\n",
    "    if t not in rt_dict:\n",
    "        rt = (t + 30).astype('datetime64[m]')\n",
    "        rt_dict[t] = rt\n",
    "    rt = rt_dict[t]\n",
    "    grid_data_assim[grid_id][rt].update(e)\n",
    "    grid_data_assim[grid_id][rt].update({'location': d['location']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    \"grid_data_assim = defaultdict(lambda: defaultdict(dict))\\nfor d in all_data_assim:\\n    for grid_id, e in d['d1'].items():\\n        process(d, e)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAssim(grid_id, v):\n",
    "    df = pd.DataFrame.from_dict(v, orient='index')\n",
    "\n",
    "    # tz\n",
    "    assert df.location.nunique() == 1\n",
    "    tz = loc_tz_dict[df.location.unique()[0]]\n",
    "    df.index = df.index.tz_localize('UTC').tz_convert(tz).floor('1d')\n",
    "    df.index.name = 'datetime'\n",
    "    df.drop(columns='location', inplace=True)\n",
    "    df = df.groupby(df.index).mean()  # .sort_index()#.head(10)\n",
    "\n",
    "    # fill\n",
    "    t = pd.to_datetime(submission.datetime.min()).tz_convert(tz).floor('1d')\n",
    "    tf = pd.to_datetime(submission.datetime.max()).tz_convert(tz).floor('1d')\n",
    "    df_t = set(df.index)\n",
    "    extra_times = []\n",
    "    while t <= tf:\n",
    "        if t not in df_t:\n",
    "            extra_times.append(t)\n",
    "        t += datetime.timedelta(days=1)\n",
    "    df_extra = pd.DataFrame(\n",
    "        np.nan,\n",
    "        index=pd.Series(extra_times),\n",
    "        columns=df.columns)\n",
    "    if not df_extra.empty:\n",
    "        df = pd.concat([df, df_extra]).sort_index()\n",
    "    else:\n",
    "        # Optionally, handle the case where df_extra is empty\n",
    "        # For example, you might simply want to ensure df is sorted by index\n",
    "        df = df.sort_index()\n",
    "\n",
    "    # ewm\n",
    "    n = 1\n",
    "    df_ewm1 = df.ewm(span=n).mean().astype(np.float32).fillna(df.mean())\n",
    "    df_ewm1.columns = [c + '_{}day'.format(n) for c in df.columns]\n",
    "    n = 2\n",
    "    df_ewm2 = df.ewm(span=n).mean().astype(np.float32).fillna(df.mean())\n",
    "    df_ewm2.columns = [c + '_{}day'.format(n) for c in df.columns]\n",
    "    n = 5\n",
    "    df_ewm5 = df.ewm(span=n).mean().astype(np.float32).fillna(df.mean())\n",
    "    df_ewm5.columns = [c + '_{}day'.format(n) for c in df.columns]\n",
    "    raw_df = df\n",
    "    df = pd.concat((df_ewm1, df_ewm2, df_ewm5,\n",
    "                    ), axis=1)\n",
    "\n",
    "    # compile\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    df.insert(0, 'grid_id', grid_id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    'all_dfs = Parallel(os.cpu_count())(delayed(processAssim)(grid_id, v)\\n                                   for grid_id, v in grid_data_assim.items() )\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assim = pd.concat(all_dfs)\n",
    "assim.index.name = 'datetime'\n",
    "assim = assim.reset_index().set_index(['datetime', 'grid_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data_assim, grid_data_assim, all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time', '', \"all_data_tropomi = loadFiles('cache/tropomi-fine.tar')\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_data_tropomi = defaultdict(list)\n",
    "for d in all_data_tropomi:\n",
    "    for grid_id, e in d['d1'].items():\n",
    "        # e = blend(e)#e.copy()\n",
    "        e['datetime'] = d['time_end']\n",
    "        e['location'] = d['location']\n",
    "        grid_data_tropomi[grid_id].append(e)\n",
    "    # break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', \"all_dfs = []\\nfor grid_id, v in grid_data_tropomi.items():\\n    df = pd.DataFrame(v)\\n\\n    # tz\\n    assert df.location.nunique() == 1 \\n    tz = loc_tz_dict[df.location.unique()[0]]\\n    df.datetime = pd.to_datetime(df.datetime).dt.tz_convert(tz).dt.floor('1d')\\n    df.drop(columns = 'location', inplace = True)\\n\\n    # group\\n    for col in [c for c in df.columns if '_mean' in c]:\\n        ct = np.where(df[col].isnull(), 0, df[col.replace('_mean', '_count')] )\\n        df[col.replace('_mean', '_sum')] = df[col] * ct\\n        df[col.replace('_mean', '_count')] = ct\\n    df = df.groupby(df.datetime).sum()#.sort_index()#.head(10)\\n    \\n\\n    for col in [c for c in df.columns if '_mean' in c]:\\n        df[col] = (df[col.replace('_mean', '_sum')] \\n                                / df[col.replace('_mean', '_count')]\\n                    )\\n    # filter\\n    df = df[[c for c in df.columns if\\n             ('.' in  c) and \\n             ( ( '_mean' in c ) or ('column_stdev' in c) )\\n            ]]\\n    \\n    # fill\\n    t = pd.to_datetime(submission.datetime.min()).tz_convert(tz).floor('1d')\\n    tf = pd.to_datetime(submission.datetime.max()).tz_convert(tz).floor('1d')\\n    df_t = set(df.index)\\n    extra_times = []\\n    while t <= tf:\\n        if t not in df_t:\\n            extra_times.append(t)\\n        t += datetime.timedelta(days = 1)\\n    df_extra = pd.DataFrame(np.nan, index = pd.Series(extra_times), columns = df.columns )\\n    if not df_extra.empty:\\n        df = pd.concat([df, df_extra]).sort_index()\\n    else:\\n        # Optionally, handle the case where df_extra is empty\\n        # For example, you might simply want to ensure df is sorted by index\\n        df = df.sort_index()\\n        \\n    # clip\\n    sigma = 4\\n    high = df.ewm(span = 100).mean() + sigma * df.ewm(span = 100).std().fillna(100000)\\n    df = df.clip(0, None)\\n    df = np.minimum(df, high)\\n    \\n    # ewm     \\n    n = 1\\n    df_ewm1 = df.ewm(span = n).mean().astype(np.float32).fillna(df.mean())\\n    df_ewm1.columns = [c + '_{}day'.format(n) for c in df.columns]\\n\\n    n = 3\\n    df_ewm3 = df.ewm(span = n).mean().astype(np.float32).fillna(df.mean())\\n    df_ewm3.columns = [c + '_{}day'.format(n) for c in df.columns]\\n    \\n    raw_df = df\\n    df = pd.concat((df_ewm1, df_ewm3, \\n                   ), axis = 1)\\n    \\n    # compile\\n    df.index = df.index.tz_localize(None)\\n    df.insert(0, 'grid_id', grid_id)\\n    all_dfs.append(df)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropomi = pd.concat(all_dfs)\n",
    "tropomi.index.name = 'datetime'\n",
    "tropomi = tropomi.reset_index().set_index(['datetime', 'grid_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data_tropomi, grid_data_tropomi, all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', '# downloadFiles(ifs_files)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loadData(f):<br>\n",
    "    s3 = boto3.client('s3')<br>\n",
    "    zd = zstd.ZstdDecompressor()<br>\n",
    "    data = pickle.loads(zd.decompress(s3.get_object(Bucket = 'projects-v', Key = f)['Body'].read()))<br>\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ifs_files = sorted(listFiles('aqi/ifs/'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ifs_tags = [<br>\n",
    "    '128_015_aluvp', '128_134_sp', '128_136_tcw',<br>\n",
    "    # '128_137_tcwv',<br>\n",
    "'128_164_tcc',<br>\n",
    " '128_165_10u',<br>\n",
    "'128_166_10v',<br>\n",
    " '128_167_2t',<br>\n",
    "'128_168_2d',<br>\n",
    "'128_206_tco3',<br>\n",
    "'228_246_100u',<br>\n",
    "'228_247_100v',<br>\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ifs_files = [f for f in ifs_files # if any(z in f for z in ifs_tags)<br>\n",
    "                    if '.oper.fc'  in f<br>\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(ifs_files)#[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loadData(f):<br>\n",
    "    s3 = boto3.client('s3')<br>\n",
    "    data = s3.get_object(Bucket = 'projects-v', Key = f)['Body'].read()<br>\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    '# all_data_ifs = Parallel(os.cpu_count() * 3)(delayed(loadData)(d) for d in \\n#                         [f for f in ifs_files[::1] ])\\n# # data = flatten(all_data)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', \"all_data_ifs = loadFiles('cache/ifs.tar')\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tarfile.open('cache/ifs.tar', \"w\") as tar:<br>\n",
    "    for file, data in zip(ifs_files[:], all_data_ifs[:]):<br>\n",
    "        t = tarfile.TarInfo(name = file)<br>\n",
    "        t.size = len(data)<br>\n",
    "        b = io.BytesIO(data)<br>\n",
    "        b.seek(0)<br>\n",
    "        tar.addfile(<br>\n",
    "            t, b)<br>\n",
    "        # print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    \"grid_data_ifs = defaultdict(lambda: defaultdict(dict))\\nfor d in all_data_ifs:\\n    for dt, v in d.items():\\n        for grid_id, e in v.items():\\n            # e['timezone'] = grid_tz_dict[grid_id]#d['location']\\n            grid_data_ifs[grid_id][dt[0]].update(e)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', \"all_dfs = []\\n# def compileIfs(grid_id, v):\\nfor grid_id, v in grid_data_ifs.items():\\n    df = pd.DataFrame(v).T\\n    df.index.name = 'datetime'\\n    # df.sort_index(inplace = True)\\n    df = df.reset_index()\\n    df.sort_values('datetime', inplace = True)\\n    \\n    # break;\\n    # extra_rows = []\\n    # t = df.datetime.min()\\n    # timestamps = set(df.datetime)\\n    # t_max = df.datetime.max()\\n    # while t < t_max:#max(timestamps):#df.datetime.max():\\n    #     t_next = t + datetime.timedelta(seconds = 60 * 60 * 12) \\n    #     if t_next not in timestamps:\\n    #         if grid_id == '1X116': print(t_next)\\n    #         extra = df[df.datetime == t].copy() #if not last else last.copy()\\n    #         extra.datetime = t_next\\n    #         extra_rows.append(extra)\\n    #         # last = extra\\n    #     # else:\\n    #         # last = None\\n    #     t = t_next;\\n    # #if len(extra_rows) > 1 and grid_id == '1X116': \\n    # #    print('{} extra rows'.format(len(extra_rows)))\\n    # df = pd.concat((df, *extra_rows))\\n    \\n    df.set_index('datetime', inplace = True)    \\n\\n    # df = addExtraRows(df.reset_index()).set_index('datetime')    \\n    df = df.ewm(span = 2 * 2).mean().astype(np.float32)\\n    df.columns = [c + '_{}day'.format(2) for c in df.columns]\\n\\n        \\n    # compile\\n    df.index = df.index.tz_localize(None)\\n    df.drop(columns = [c for c in df.columns if 'vapour' in c \\n                       or 'mean0.05' in c \\n                       or 'mean0.2' in c\\n                       or 'mean1' in c\\n                       or 'roughness' in c\\n                       or 'albedo' in c\\n                      ], inplace = True)\\n    df.columns = ['ifs_'+ c for c in df.columns]\\n    df.insert(0, 'grid_id', grid_id)\\n    # return df;\\n\\n    all_dfs.append(df)\\n    # break;\\n    # break;\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_dfs = Parallel(#os.cpu_count() * 2<br>\n",
    "                  1)(delayed(compileIfs)(grid_id, v)<br>\n",
    "                           for grid_id, v in grid_data_ifs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifs = pd.concat(all_dfs)\n",
    "ifs.index.name = 'datetime'\n",
    "ifs = ifs.reset_index().set_index(['datetime', 'grid_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 10, -1]:\n",
    "    df = all_dfs[i]\n",
    "    df['grid_id'] = df['grid_id'].astype(str)\n",
    "    grid['grid_id'] = grid['grid_id'].astype(str)  # Ensure grid_id is string\n",
    "    try:\n",
    "        location = grid.set_index('grid_id').location[df.grid_id.iloc[0]]\n",
    "        print(location)\n",
    "    except KeyError:\n",
    "        print(f\"Location for grid_id {df['grid_id'].iloc[0]} not found.\")\n",
    "        location = \"Unknown Location\"  # Default or placeholder location\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    # plt.matshow(df_numeric.corr())\n",
    "    # plt.colorbar()\n",
    "    # plt.title(location)\n",
    "    # plt.show()  # Ensure the plot is shown for each iteration\n",
    "    # plt.matshow(df.corr()); plt.colorbar(); plt.title(str(location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[1::6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data_ifs, grid_data_ifs, all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gfs_files = listFiles('aqi/gfs-5/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(gfs_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tarfile.open('cache/gfs-5.tar', \"w\") as tar:<br>\n",
    "    tar.add('cache/gfs-5/', arcname=os.path.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    \"# all_data_gfs = Parallel(os.cpu_count() * 3)(delayed(loadData)(d) for d in \\n#                         [f for f in gfs_files if 'f000' in f])\\n# all_data_gfs = flatten(all_data_gfs)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    \"all_data_gfs = loadFiles('cache/gfs-5.tar')\\nall_data_gfs = flatten(all_data_gfs)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data_gfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic(\n",
    "    'time',\n",
    "    '',\n",
    "    \"points = defaultdict(lambda: defaultdict(list))\\nfor e in all_data_gfs:# [e for e in h if e[1] == city]:\\n    t = datetime.datetime.strptime(e[0][-12:-2], '%Y%m%d%H')\\n    n = ':'.join(e[0].split(':')[1:2])\\n    city = e[1]\\n    \\n    arr = e[3] \\n    arr2 = cv2.resize(arr, None, fx = 5, fy = 5,  )\\n    \\n    for grid_id, x, y in coords[city]:\\n        v = arr2[ int(round(( e[2][0][1] - y ) / 0.05 + 2)), \\n            int(round( ( (x % 360) - e[2][1][0] )/0.05 + 2 ) ) ]\\n        points[grid_id][n].append((t, v, e[-1]))\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addExtraRows(df):\n",
    "    extra_rows = []\n",
    "    t = df.datetime.min()\n",
    "    timestamps = set(df.datetime)\n",
    "    t_max = df.datetime.max()\n",
    "    while t < t_max:  # max(timestamps):#df.datetime.max():\n",
    "        t_next = t + datetime.timedelta(seconds=60 * 60 * 6)\n",
    "        if t_next not in timestamps:\n",
    "            extra = df[df.datetime == t].copy() if not last else last.copy()\n",
    "            extra.datetime = t_next\n",
    "            extra_rows.append(extra)\n",
    "            last = extra\n",
    "        else:\n",
    "            last = None\n",
    "        t = t_next\n",
    "    if len(extra_rows) > 1:\n",
    "        print('{} extra rows'.format(len(extra_rows)))\n",
    "    return pd.concat((df, *extra_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for grid_point, grid_data in points.items():\n",
    "    dfs = []\n",
    "    for label, d in grid_data.items():\n",
    "        if 'Volumetric soil moisture content' in label:\n",
    "            continue\n",
    "        df = pd.DataFrame(\n",
    "            d,\n",
    "            columns=[\n",
    "                'datetime',\n",
    "                label +\n",
    "                '_local',\n",
    "                label +\n",
    "                '_city'])\n",
    "        df.sort_values('datetime', inplace=True)\n",
    "        dfs.append(df.set_index('datetime'))\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df = addExtraRows(df.reset_index()).set_index('datetime')\n",
    "    df = df.ewm(span=4 * 3).mean().astype(np.float32)\n",
    "    df.columns = [c + '_{}day'.format(3) for c in df.columns]\n",
    "    df['grid_id'] = grid_point\n",
    "    all_dfs.append(df)\n",
    "gfs = pd.concat(all_dfs)\n",
    "gfs = gfs.reset_index().set_index(['datetime', 'grid_id'])\n",
    "#\n",
    "# print(df.corr().iloc[0, 1], label)\n",
    "# break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data_gfs, points, all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['dayofweek'] = labels.datetime.dt.dayofweek.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['dayinyear'] = labels.datetime.dt.dayofyear.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addGFS(labels):\n",
    "    ldo = pd.to_datetime(labels.datetime).dt.floor('6h').dt.tz_localize(None)\n",
    "    df = pd.concat((labels,\n",
    "                    * [2 / 3 * gfs.reindex([ldo\n",
    "                                            + datetime.timedelta(seconds=60 * 60 * 12),\n",
    "                                            labels.grid_id]).reset_index(drop=True)\n",
    "                        + 1 / 3 * gfs.reindex([ldo\n",
    "                                               + datetime.timedelta(seconds=60 * 60 * 18),\n",
    "                                               labels.grid_id]).reset_index(drop=True)\n",
    "                       # for k, v in gfs_ewms.items()\n",
    "                       ],\n",
    "                    ), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gfs.groupby(['datetime', 'grid_id']).nunique().mean(axis = 1).sort_values()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifsoffset12(c): return c.replace('ifs', 'ifs12')\n",
    "def ifsoffset0(c): return c.replace('ifs', 'ifs0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addIFS(labels):\n",
    "    ldo = pd.to_datetime(labels.datetime).dt.floor('12h').dt.tz_localize(None)\n",
    "    df = pd.concat((labels,\n",
    "                    * [ifs.reindex([ldo\n",
    "                                    + datetime.timedelta(seconds=60 * 60 * 12),\n",
    "                                    labels.grid_id]).reset_index(drop=True).rename(\n",
    "                        columns=ifsoffset12),\n",
    "                        ifs.reindex([ldo                            # + datetime.timedelta(seconds = 60 * 60 * 0)\n",
    "                                     ,\n",
    "                                     labels.grid_id]).reset_index(drop=True).rename(\n",
    "                        columns=ifsoffset0)\n",
    "                       # for k, v in gfs_ewms.items()\n",
    "                       ],\n",
    "                    ), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSat(labels, sat):\n",
    "    tza = labels.datetime.copy()\n",
    "    for location in labels.location.unique():\n",
    "        t = pd.to_datetime(labels.datetime).dt.tz_convert(tz_dict[location])\n",
    "        t = t.dt.floor('1d').dt.tz_localize(None)\n",
    "        tza = np.where(labels.location == location, t, tza)\n",
    "    tza = pd.to_datetime(pd.Series(tza, index=labels.index))\n",
    "    df = pd.concat((labels,\n",
    "                    sat.reindex([tza,\n",
    "                                 labels.grid_id]).reset_index(drop=True)\n",
    "                    ), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = addGFS(labels)\n",
    "all_data = addIFS(all_data)\n",
    "all_data = addSat(all_data, 'maiac' if dataset == 'pm' else tropomi)\n",
    "if ASSIM:\n",
    "    all_data = addSat(all_data, assim)\n",
    "# all_data = addSat(all_data, misr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_data[[c for c in all_data.columns if c not in [\n",
    "    'datetime', 'value']]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'7334C' - > '7F1D1'<br>\n",
    "'HANW9' - > 'WZNCR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'tg':\n",
    "    np.random.seed(SEED)\n",
    "    x.loc[(x.grid_id == '7334C') & (\n",
    "        np.random.random(len(x)) < 0.1), 'grid_id'] = '7F1D1'\n",
    "    x.loc[(x.grid_id == 'HANW9') & (\n",
    "        np.random.random(len(x)) < 0.1), 'grid_id'] = 'WZNCR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grid_id = x.grid_id.astype('category')\n",
    "x.location = x.location.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_data.value.astype(np.float32)\n",
    "d = all_data.datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_data, open('cache/all_data_{}.pkl'.format(dataset), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns = [c.replace(':', '_') for c in x.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_params = {\n",
    "    'alpha': [1e-2, 3e-2, 1e-1, 0.3, 1, 3, ],\n",
    "    'l1_ratio': [0.01, 0.03, 0.1, 0.2, 0.5, 0.8, 0.9, ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold():\n",
    "    def __init__(self, n_splits=5, gap=30):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "    def get_n_splits(self, X, y=None, groups=None): return self.n_splits\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = groups.sort_values()\n",
    "        X = X.reindex(groups.index)  # sort_values(groups)\n",
    "        y = y.reindex(X.index)\n",
    "        X, y, groups = sklearn.utils.indexable(X, y, groups)\n",
    "        indices = np.arange(len(X))\n",
    "        n_splits = self.n_splits\n",
    "        for i in range(n_splits):\n",
    "            test = indices[i * len(X) // n_splits: (i + 1)\n",
    "                           * len(X) // n_splits]  # .index\n",
    "            train = indices[(groups <= groups.iloc[test].min() -\n",
    "                             datetime.timedelta(days=self.gap)) | (groups >= groups.iloc[test].max() +\n",
    "                                                                   datetime.timedelta(days=self.gap))]  # .index\n",
    "            yield train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedPurgedKFold():\n",
    "    def __init__(self, n_splits=5, n_repeats=1, gap=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_repeats = n_repeats\n",
    "        self.gap = gap\n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        return self.n_splits * self.n_repeats + \\\n",
    "            self.n_repeats * (self.n_repeats - 1) // 2\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for i in range(self.n_repeats):\n",
    "            for f in PurgedKFold(\n",
    "                    self.n_splits + i,\n",
    "                    gap=self.gap if self.gap else None).split(\n",
    "                    X,\n",
    "                    y,\n",
    "                    groups):\n",
    "                yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posRMSE(y, y_pred):\n",
    "    return mean_squared_error(\n",
    "        y,\n",
    "        y_pred.clip(\n",
    "            0,\n",
    "            None) *\n",
    "        2 /\n",
    "        3 +\n",
    "        y_pred *\n",
    "        1 /\n",
    "        3) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pRMSE = make_scorer(posRMSE, greater_is_better=False)\n",
    "SCORING = pRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_WIPE = 0  # len(x.columns) // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runENet(drop_cols=[], verbose=1):\n",
    "    all_y_enet = []\n",
    "    all_y_pred_enet = []\n",
    "    enet_clfs = []\n",
    "    enet_scalers = []\n",
    "    for location in x.location.unique():\n",
    "        if verbose > 0:\n",
    "            print(location)\n",
    "        x_loc = x[x.location == location].drop(columns=drop_cols)\n",
    "        y_loc = y.reindex(x_loc.index)\n",
    "        d_loc = d.reindex(x_loc.index)\n",
    "        folds = list(\n",
    "            PurgedKFold(\n",
    "                4 if dataset == 'pm' else 3).split(\n",
    "                x_loc,\n",
    "                y_loc,\n",
    "                d_loc))\n",
    "        folds += [(np.arange(0, len(x_loc)), [])] * 2\n",
    "        for train_fold, test_fold in folds:\n",
    "            y_preds = []\n",
    "            for i in range(8):\n",
    "                scaler = StandardScaler()\n",
    "                clf = ElasticNet(\n",
    "                    max_iter=50000,\n",
    "                    tol=1e-3,\n",
    "                    selection='random',\n",
    "                    precompute=True,\n",
    "                    random_state=datetime.datetime.now().microsecond)\n",
    "                model = RandomizedSearchCV(clf, enet_params,\n",
    "                                           scoring=SCORING,\n",
    "                                           # 'neg_root_mean_squared_error',\n",
    "                                           cv=RepeatedPurgedKFold(random.randrange(3, 6),\n",
    "                                                                  n_repeats=random.randrange(\n",
    "                                                                      2, 4),\n",
    "                                                                  gap=random.randrange(60, 120)),\n",
    "                                           n_iter=random.randrange(4, 7),\n",
    "                                           random_state=datetime.datetime.now().microsecond)\n",
    "                # subset = train_fold#[:s].tolist() + train_fold[s +\n",
    "                # l:].tolist()\n",
    "                l = random.randrange(0, len(train_fold) // 10)\n",
    "                s = random.randrange(0, len(train_fold) - l)\n",
    "                subset = train_fold[:s].tolist() + train_fold[s + l:].tolist()\n",
    "                xt = x_loc.iloc[subset, 2:].copy()\n",
    "                # for c in misr.columns:\n",
    "                if dataset == 'pm':\n",
    "                    sample_cols = [\n",
    "                        c for c in xt.columns if any(\n",
    "                            z in c for z in [\n",
    "                                'precision',\n",
    "                                'air_mass',\n",
    "                                'stdev'] + [\n",
    "                                'no2',\n",
    "                                'so2',\n",
    "                                'co',\n",
    "                                'o3',\n",
    "                                'pm25_rh35_gcc',\n",
    "                                'ifs'])]\n",
    "                    xt[random.sample(sample_cols, k=random.randrange(\n",
    "                        len(sample_cols) * 1 // 3, len(sample_cols) * 2 // 3))] = 0\n",
    "                if dataset == 'tg':\n",
    "                    if random.random() < 0.5:\n",
    "                        xt[[c for c in xt.columns if 'precision' in c]] = 0\n",
    "                    # if random.random() < 0.5: xt[[c for c in xt.columns if 'mean1' in c or 'mean2' in c]] = 0\n",
    "                # xt[misr.columns] = 0\n",
    "                # fj_drop =\n",
    "                # if random.random() < {'Delhi': 0.5, 'Taipei': 0.2}.get(location, 0):\n",
    "                #     xt[[c for c in xt.columns if 'FineMode' in c or 'Injection' in c]] = 0\n",
    "                # # if random.random() < {'Delhi': 0.2, 'Taipei': 0.1}.get(location, 0):\n",
    "                # #     xt[[c for c in xt.columns if 'Optical' in c]] = 0\n",
    "                # if random.random() < {'Delhi': 0.85, 'Taipei': 0.5}.get(location, 0.2):\n",
    "                #     xt[[c for c in xt.columns if c.startswith('ifs')]] = 0\n",
    "                # xt = x_loc.iloc[subset].copy()\n",
    "                xt[random.choices(xt.columns[2:], k=int(\n",
    "                    round(random.random() * COLUMN_WIPE)))] = 0\n",
    "                if random.random() < 0.3:\n",
    "                    xt['dayinyear'] = 0\n",
    "                model.fit(\n",
    "                    pd.DataFrame(\n",
    "                        scaler.fit_transform(xt).astype(\n",
    "                            np.float32),\n",
    "                        xt.index,\n",
    "                        xt.columns),\n",
    "                    y_loc.iloc[subset],\n",
    "                    groups=d_loc.iloc[subset])\n",
    "                enet_clfs.append(model.best_estimator_)\n",
    "                enet_scalers.append(scaler)\n",
    "                if i == 0 and verbose > 0:\n",
    "                    display(\n",
    "                        pd.DataFrame(\n",
    "                            model.cv_results_).sort_values('rank_test_score').drop(\n",
    "                            columns='params'))\n",
    "                if len(test_fold) > 0:\n",
    "                    y_pred = pd.Series(model.predict(\n",
    "                        pd.DataFrame(scaler.transform(x_loc.iloc[test_fold, 2:]).astype(np.float32),\n",
    "                                     columns=x_loc.columns[2:])),  # .clip(0, None),\n",
    "                        index=y_loc.iloc[test_fold].index)\n",
    "                    y_preds.append(y_pred)\n",
    "            if len(test_fold) > 0:\n",
    "                y_pred = pd.concat(y_preds)\n",
    "                y_pred = y_pred.clip(\n",
    "                    0,\n",
    "                    None).groupby(\n",
    "                    y_pred.index).mean().clip(\n",
    "                    0,\n",
    "                    None)\n",
    "                y_pred = (y_pred.groupby(y_pred.index).mean())  # *2/3\n",
    "                # + y_pred.groupby(y_pred.index).median() * 1/3 )\n",
    "                all_y_enet.append(y_loc.iloc[test_fold])\n",
    "                all_y_pred_enet.append(y_pred)\n",
    "                if verbose >= 1:\n",
    "                    print(location, round(np.corrcoef(\n",
    "                        y_pred, all_y_enet[-1])[0, 1], 4))\n",
    "                if verbose >= 3:\n",
    "                    all_y_enet[-1].reset_index(\n",
    "                        drop=True).ewm(span=10).mean().plot()\n",
    "                    plt.plot(y_pred.reset_index(drop=True))\n",
    "                    plt.title(location)\n",
    "                    plt.figure()\n",
    "                    print((x_loc.iloc[subset, 2:].std().clip(1, 1) *\n",
    "                           model.best_estimator_.coef_).sort_values())\n",
    "    return all_y_enet, all_y_pred_enet, enet_clfs, enet_scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_enet, all_y_pred_enet, enet_clfs, enet_scalers = runENet(verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('clfs_{}'.format(dataset), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((all_y_enet, all_y_pred_enet, enet_clfs, enet_scalers),\n",
    "            open('clfs_{}/enet_clfs_{}.pkl'.format(\n",
    "                 dataset, run_label), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(3):<br>\n",
    "    pd.Series(np.mean([e.coef_ for e in<br>\n",
    "                   enet_clfs[len(enet_clfs) * i//3:len(enet_clfs) * (i + 1)//3]],<br>\n",
    "        axis = 0), index = x.columns[2:]).sort_values().plot(kind = 'barh',<br>\n",
    "                                                                figsize = (10, x.shape[1]// 4),<br>\n",
    "                                                                title = list(x.location.unique())[i], )<br>\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(round(np.corrcoef( pd.concat(all_y_pred_enet).clip(0, None),#.reindex(df.index),<br>\n",
    "                            pd.concat(all_y_enet)#.reindex(df.index)<br>\n",
    "           )[0, 1], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "for location, df in x.groupby('location'):\n",
    "    c = np.corrcoef(pd.concat(all_y_pred_enet).clip(0, None).reindex(df.index),\n",
    "                    pd.concat(all_y_enet).reindex(df.index))[0, 1]\n",
    "    cs.append(c)\n",
    "    print(location, round(c, 3))\n",
    "print('\\nBlend:', round(np.mean(cs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    # [ 150, 200, 300, ],\n",
    "    'n_estimators': np.arange(200, 400, 10) if dataset == 'pm' else np.arange(300, 600, 20),\n",
    "    # [0.03, 0.05, 0.07, ],\n",
    "    'learning_rate': np.arange(0.01, 0.04, 0.003) if dataset == 'pm' else np.arange(0.01, 0.061, 0.005),\n",
    "    # [5, 7, 10, 15, 20,],\n",
    "    'num_leaves': np.arange(4, 30) if dataset == 'pm' else np.arange(10, 30),\n",
    "    'min_child_weight': np.arange(0.02, 0.1, 0.01),  # [  0.1, 0.2, ],\n",
    "    'min_child_samples': [140, 170, 200, 250, 300, 400, 500, 600, 700, 850, 1000, 1400, ]\n",
    "    if dataset == 'pm' else\n",
    "                         [30, 40, 50, 60, 80, 100, 120, 150,\n",
    "                             170, 200, 300, 500, 700, ],\n",
    "    'reg_lambda': [0, 1e-5, 1e-4, 1e-3, 1e-2, 0.1,],\n",
    "    'reg_alpha': [0, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, ],\n",
    "    'linear_tree': [True, ],\n",
    "    'subsample': np.arange(0.4, 0.901, 0.05),  # [0.3, 0.5,  0.8],\n",
    "    'subsample_freq': [1],\n",
    "    # [0.5, 0.8, ],#0.3, 0.5, 0.8],\n",
    "    'colsample_bytree': np.arange(0.3, 0.71, 0.05) if dataset == 'pm' else np.arange(0.2, 0.81, 0.05),\n",
    "    # [0.5, 0.8, ],#0.3, 0.5, 0.8],\n",
    "    'colsample_bynode': np.arange(0.4, 1.01, 0.05) if dataset == 'pm' else np.arange(0.2, 1.01, 0.05),\n",
    "    'linear_lambda': [1e-3, 3e-3, 1e-2, 3e-2, 0.1,],\n",
    "    # 'max_bins': np.arange(120, 400, 20),\n",
    "    # 'min_data_in_bin': np.exp(np.arange(np.log(3), np.log(12), 0.1)).astype(int), #np.arange(2, 10),# [2, 3, 4, 5, 10],\n",
    "    # [ 192,\n",
    "    # 255, 255, 384, 512],\n",
    "    'min_data_per_group': [10, 20, 50, 100],\n",
    "    'max_cat_threshold': [8, 16, 32, ],\n",
    "    'cat_l2': [0.1, 1, 10],  # if dataset == 'pm' else [1e-3, 1e-2, 1e-1],\n",
    "    'cat_smooth': [0.1, 1, 10],  # if dataset == 'pm' else [1e-3, 1e-2, 1e-1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST = False\n",
    "GAUSSIAN = 0.05  # if dataset == 'pm' else 0.1\n",
    "COLUMN_WIPE = len(x.columns) // 5  # if dataset == 'pm' else 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLGB(drop_cols=[], verbose=1, n_bags=8, ):\n",
    "    all_y_lgb = []\n",
    "    all_y_pred_lgb = []\n",
    "    tidx = 0\n",
    "    lgb_clfs = []\n",
    "    lgb_scalers = []\n",
    "    for location in x.location.unique():\n",
    "        random.seed(datetime.datetime.now().microsecond)\n",
    "        np.random.seed(datetime.datetime.now().microsecond)\n",
    "        if FAST and location != 'Delhi':\n",
    "            continue\n",
    "        if verbose > 0:\n",
    "            print(location)\n",
    "        x_loc = x[x.location == location].drop(\n",
    "            columns=drop_cols)  # .iloc[:, :4]\n",
    "        y_loc = y.reindex(x_loc.index)\n",
    "        d_loc = d.reindex(x_loc.index)\n",
    "        folds = list(\n",
    "            (PurgedKFold(4) if dataset == 'pm' else PurgedKFold(\n",
    "                3, gap=20)) .split(\n",
    "                x_loc, y_loc, d_loc))\n",
    "        folds += [(np.arange(0, len(x_loc)), [])] * 3\n",
    "        for train_fold, test_fold in folds:\n",
    "            y_preds = []\n",
    "            for i in range(n_bags):  # if location == 'Delhi' else 4):\n",
    "                model = RandomizedSearchCV(lgb.LGBMRegressor(seed=datetime.datetime.now().microsecond,\n",
    "                                                             # n_jobs = 2, #\n",
    "                                                             # os.cpu_count()\n",
    "                                                             ), lgb_params,\n",
    "                                           cv=RepeatedPurgedKFold(random.randrange(3, 6),\n",
    "                                                                  n_repeats=random.randrange(\n",
    "                                                                      1, 3),\n",
    "                                                                  gap=random.randrange(60, 120)),\n",
    "                                           n_iter=random.randrange(3, 5),\n",
    "                                           scoring=SCORING,\n",
    "                                           n_jobs=-1,  # os.cpu_count(),\n",
    "                                           random_state=datetime.datetime.now().microsecond)\n",
    "                l = random.randrange(0, len(train_fold) // 10)\n",
    "                s = random.randrange(0, len(train_fold) - l)\n",
    "                subset = train_fold[:s].tolist() + train_fold[s + l:].tolist()\n",
    "                xt = x_loc.iloc[subset].copy()\n",
    "                xt.iloc[:, 2:] += (GAUSSIAN * random.random() *\n",
    "                                   np.random.default_rng().standard_normal(size=xt.iloc[:, 2:].shape)\n",
    "                                   * xt.iloc[:, 2:].std().values[None, :])\n",
    "                for c in random.choices(xt.columns[2:], k=int(\n",
    "                        round(random.random() * COLUMN_WIPE))):\n",
    "                    xt[c] = 0\n",
    "                if dataset == 'pm' and random.random() < 0.3:\n",
    "                    xt['dayinyear'] = 0\n",
    "                sample_cols = [\n",
    "                    c for c in xt.columns if any(\n",
    "                        z in c for z in [\n",
    "                            'precision',\n",
    "                            'air_mass',\n",
    "                            'stdev'] + [\n",
    "                            'no2',\n",
    "                            'so2',\n",
    "                            'co',\n",
    "                            'o3',\n",
    "                            'pm25_rh35_gcc',\n",
    "                            'ifs'])]\n",
    "                xt[random.sample(sample_cols, k=random.randrange(\n",
    "                    len(sample_cols) * 1 // 3, len(sample_cols) * 2 // 3))] = 0\n",
    "                # if random.random() < {'Delhi': 0.7, 'Taipei': 0.2}.get(location, 0):\n",
    "                #     xt[[c for c in xt.columns if 'FineMode' in c or 'Injection' in c]] = 0\n",
    "                # if random.random() < {'Delhi': 0.1, 'Taipei': 0.1}.get(location, 0):\n",
    "                #     xt[[c for c in xt.columns if 'Optical' in c]] = 0\n",
    "                # if random.random() < {'Delhi': 1.0, 'Taipei': 0.2}.get(location, 0.2):\n",
    "                #     xt[[c for c in xt.columns if c.startswith('ifs')]] = 0\n",
    "                # if random.random() < {'Delhi': 0.9,}.get(location, 0.12):\n",
    "                #     xt[[c for c in xt.columns if any(z + '_' in c for z in\n",
    "                #          ['no2', 'so2', 'co', 'o3', 'pm25_rh35_gcc',  ] ) ]] = 0\n",
    "                scaler = StandardScaler()\n",
    "                xt.iloc[:, 2:] = scaler.fit_transform(\n",
    "                    xt.iloc[:, 2:]).astype(np.float32)\n",
    "                # for i in xt, y_loc.iloc[subset], d_loc.iloc[subset]:\n",
    "                #     if i\n",
    "                y_loc_subset = y_loc.iloc[subset].astype(np.int64)\n",
    "                d_loc_subset = d_loc.iloc[subset].astype(np.int64) # //1e9\n",
    "                print(f'xt = {xt}')\n",
    "                print(f'y_loc.iloc[subset] = {y_loc_subset}')\n",
    "                print(f'd_loc.iloc[subset] = {d_loc_subset}')\n",
    "                model.fit(xt, y_loc_subset, groups=d_loc_subset,\n",
    "                         )\n",
    "                lgb_clfs.append(model.best_estimator_)\n",
    "                lgb_scalers.append(scaler)\n",
    "                if len(test_fold) > 0:\n",
    "                    xtst = x_loc.iloc[test_fold].copy()\n",
    "                    xtst.iloc[:, 2:] = scaler.transform(\n",
    "                        xtst.iloc[:, 2:]).astype(np.float32)\n",
    "                    y_pred = pd.Series(\n",
    "                        model.predict(xtst),\n",
    "                        index=y_loc.iloc[test_fold].index)\n",
    "                    y_preds.append(y_pred)\n",
    "                df = pd.DataFrame(model.cv_results_).sort_values(\n",
    "                    'rank_test_score').drop(columns='params')\n",
    "                if i == 0 and verbose > 1:  # df.mean_test_score.min() < -0:\n",
    "                    display(df)\n",
    "                    print()\n",
    "            if len(test_fold) > 0:\n",
    "                y_pred = pd.concat(y_preds)\n",
    "                y_pred = (y_pred.groupby(y_pred.index).mean())  # * 2/3\n",
    "                # + y_pred.groupby(y_pred.index).median() * 1/3 )\n",
    "                all_y_lgb.append(y_loc.iloc[test_fold])\n",
    "                all_y_pred_lgb.append(y_pred)\n",
    "                if verbose >= 3:\n",
    "                    all_y_lgb[-1].reset_index(\n",
    "                        drop=True).ewm(span=10).mean().plot()\n",
    "                    plt.plot(y_pred.reset_index(drop=True), linewidth=0.8)\n",
    "                    try:\n",
    "                        plt.plot(\n",
    "                            all_y_pred_enet[tidx].clip(\n",
    "                                0, None).reset_index(\n",
    "                                drop=True))\n",
    "                    except BaseException:\n",
    "                        pass\n",
    "                    plt.title(location)\n",
    "                    plt.figure()\n",
    "                tidx += 1\n",
    "                if verbose > 0:\n",
    "                    print(location, round(np.corrcoef(\n",
    "                        y_pred, all_y_lgb[-1])[0, 1], 4))\n",
    "    return all_y_lgb, all_y_pred_lgb, lgb_clfs, lgb_scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_lgb, all_y_pred_lgb, lgb_clfs, lgb_scalers = runLGB(verbose=2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((all_y_lgb, all_y_pred_lgb, lgb_clfs, lgb_scalers),\n",
    "            open('clfs_{}/lgb_clfs_{}.pkl'.format(\n",
    "                 dataset, run_label), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print(round(np.corrcoef( pd.concat(all_y_pred_enet).clip(0, None), pd.concat(all_y_lgb))[0, 1], 3)  )<br>\n",
    "print(round(np.corrcoef( pd.concat(all_y_pred_lgb).clip(0, None), pd.concat(all_y_lgb))[0, 1], 3) )<br>\n",
    "print(round(np.corrcoef( pd.concat(all_y_pred_lgb).clip(0, None)<br>\n",
    "                  +  0.1  * pd.concat(all_y_pred_enet).clip(0, None)<br>\n",
    "                  , pd.concat(all_y_lgb))[0, 1], 4) )<br>\n",
    " # without weather ~0.79 lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "for location, df in x.groupby('location'):\n",
    "    c = np.corrcoef(\n",
    "        pd.concat(all_y_pred_lgb).clip(\n",
    "            0,\n",
    "            None).reindex(\n",
    "            df.index) +\n",
    "        0.1 *\n",
    "        pd.concat(all_y_pred_enet).clip(\n",
    "            0,\n",
    "            None).reindex(\n",
    "            df.index),\n",
    "        pd.concat(all_y_lgb).reindex(\n",
    "            df.index))[\n",
    "        0,\n",
    "        1]\n",
    "    cs.append(c)\n",
    "    print(location,\n",
    "          round(c, 3))\n",
    "print('\\nBlend: ', round(np.mean(cs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lgb_clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data_{}/submission_format.csv'.format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['dayinyear'] = pd.to_datetime(submission.datetime).dt.dayofyear\n",
    "submission['dayofweek'] = pd.to_datetime(submission.datetime).dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['location'] = grid.set_index(\n",
    "    'grid_id').location.reindex(submission.grid_id).values\n",
    "submission['location'] = submission.location.astype('category')\n",
    "submission['grid_id'] = submission.grid_id.astype(\n",
    "    x.grid_id.dtype)  # 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = addGFS(submission)\n",
    "submission = addIFS(submission)\n",
    "submission = addSat(submission, 'maiac' if dataset == 'pm' else tropomi)\n",
    "if ASSIM:\n",
    "    submission = addSat(submission, assim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(submission, open('cache/submission_{}.pkl'.format(dataset), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = submission[x.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.location.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_path = 'clfs_{}'.format(dataset)\n",
    "all_clfs = os.listdir(clf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enet_tuples = [pickle.load(open( os.path.join(clf_path, f), 'rb'))<br>\n",
    "                   for f in all_clfs if 'enet_clfs' in f] if dataset == 'pm' else []<br>\n",
    "lgb_tuples = [pickle.load(open( os.path.join(clf_path, f), 'rb'))<br>\n",
    "                   for f in all_clfs if 'lgb_clfs' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_tuples = [(all_y_enet, all_y_pred_enet, enet_clfs, enet_scalers)]\n",
    "lgb_tuples = [(all_y_lgb, all_y_pred_lgb, lgb_clfs, lgb_scalers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lgb_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enet_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.concat(flatten([e[0] for e in lgb_tuples]))\n",
    "y_true = y_true.groupby(y_true.index).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = pd.concat(flatten([e[1] for e in lgb_tuples]))\n",
    "y_pred_lgb = y_pred_lgb.groupby(y_pred_lgb.index).mean()  # * 2/3\n",
    "# + y_pred_lgb.groupby(y_pred_lgb.index).median() * 1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'pm':\n",
    "    y_pred_enet = pd.concat(flatten([e[1] for e in enet_tuples]))\n",
    "    y_pred_enet = (y_pred_enet.groupby(y_pred_enet.index).mean())\n",
    "    # + y_pred_enet.groupby(y_pred_enet.index).mean() * 1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "for location, df in x.groupby('location'):\n",
    "    c = np.corrcoef(\n",
    "        y_pred_lgb.clip(\n",
    "            0, None).reindex(\n",
    "            df.index) + 1 / 5 * (\n",
    "                y_pred_enet.clip(\n",
    "                    0, None).reindex(\n",
    "                        df.index) if dataset == 'pm' else 0), y_true.reindex(\n",
    "                            df.index))[\n",
    "                                0, 1]\n",
    "    cs.append(c)\n",
    "    print(location,\n",
    "          round(c, 3))\n",
    "print('\\nBlend: ', round(np.mean(cs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[c.n_features_in_ for c in enet_scalers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clfs, lgb_scalers, enet_clfs, enet_scalers = [], [], [], []\n",
    "for i in range(3):\n",
    "    lgb_clfs.extend(flatten([e[2][i * len(e[2]) // 3: (i + 1) * len(e[2]) // 3]\n",
    "                             for e in lgb_tuples]))\n",
    "    lgb_scalers.extend(flatten(\n",
    "        [e[3][i * len(e[3]) // 3: (i + 1) * len(e[3]) // 3] for e in lgb_tuples]))\n",
    "    enet_clfs.extend(flatten([e[2][i * len(e[2]) // 3: (i + 1) * len(e[2]) // 3]\n",
    "                              for e in enet_tuples]))\n",
    "    enet_scalers.extend(flatten(\n",
    "        [e[3][i * len(e[3]) // 3: (i + 1) * len(e[3]) // 3] for e in enet_tuples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lgb_clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_ys = []\n",
    "for clf_idx, clf in enumerate(lgb_clfs):\n",
    "    x_loc = xs[xs.location == x.location.unique()[0]].copy()  # Always use the first (and only) location\n",
    "    x_loc.iloc[:, 2:] = lgb_scalers[clf_idx].transform(x_loc.iloc[:, 2:])\n",
    "    lgb_ys.append(pd.Series(clf.predict(x_loc), index=x_loc.index))\n",
    "    lgb_y_combined = pd.concat(lgb_ys, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_ys = []\n",
    "for clf_idx, clf in enumerate(enet_clfs):\n",
    "    x_loc = xs[xs.location == x.location.unique()[0]].copy().iloc[:,2:]\n",
    "    enet_ys.append(pd.Series(clf.predict(\n",
    "        pd.DataFrame(enet_scalers[clf_idx].transform(x_loc), \n",
    "                     columns = x_loc.columns)), index = x_loc.index))\n",
    "enet_ys = pd.concat(enet_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb_ys.groupby(lgb_ys.index).std().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y = lgb_y_combined.groupby(lgb_y_combined.index).mean().clip(0, None)  # .sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_y = enet_ys.groupby(enet_ys.index).mean().clip(0, None)  # .sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if dataset == 'pm':<br>\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.scatter(enet_y, lgb_y, s= 0.1)<br>\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mcorrcoef(enet_y, lgb_y)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m4\u001b[39m) )\u001b[38;5;66;03m#, s= 0.1)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# if dataset == 'pm':<br>\n",
    "# plt.scatter(enet_y, lgb_y, s= 0.1)<br>\n",
    "print(round(np.corrcoef(enet_y, lgb_y)[0,1], 4) )#, s= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 8 if dataset == 'pm' else 100000  # lgb_y\n",
    "ys = (lgb_y * (f - 1) / f + enet_y * 1 / f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.name = 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.concat((submission[['datetime', 'grid_id']],\n",
    "                ys.reindex(submission.index)), axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[pd.to_datetime(out.datetime) < datetime.datetime(2018, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset == 'pm' and ASSIM:<br>\n",
    "#     pivot = '2018-01-10'<br>\n",
    "#     b1 = pd.read_csv('submissions_pm/blend1.csv')<br>\n",
    "#     mix = pd.concat((<br>\n",
    "#         b1[b1.datetime < pivot],<br>\n",
    "#         out[out.datetime >= pivot] ) )<br>\n",
    "#     assert out.shape == mix.shape<br>\n",
    "#     out = mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in out.grid_id.unique():<br>\n",
    "#     out[out.grid_id == g].set_index(pd.to_datetime(out[out.grid_id == g].datetime)).value.plot(<br>\n",
    "#         marker = '.', linewidth = 0.1,)# markersize = 3)<br>\n",
    "# plt.xlim(pd.to_datetime(submission.datetime).min(),<br>\n",
    "# pd.to_datetime(submission.datetime).min() + datetime.timedelta(days =<br>\n",
    "# 400));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in out.grid_id.unique():<br>\n",
    "#     out[out.grid_id == g].set_index(pd.to_datetime(out[out.grid_id == g].datetime)).value.plot(<br>\n",
    "#         marker = '.', linewidth = 0.1,)# markersize = 3)<br>\n",
    "# plt.xlim(datetime.datetime(2020, 10, 15), datetime.datetime(2021, 9, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset == 'pm':<br>\n",
    "#     sp = pd.read_csv('submissions_pm/lgb_baseline.csv')<br>\n",
    "#     sp2 = pd.read_csv('submissions_pm/lgb_2.csv')<br>\n",
    "#     g1 = pd.read_csv('submissions_pm/first_gfs.csv')<br>\n",
    "#     m1 = pd.read_csv('submissions_pm/maiac.csv')<br>\n",
    "#     m2 = pd.read_csv('submissions_pm/maiac2.csv')<br>\n",
    "#     m3 = pd.read_csv('submissions_pm/maiac3.csv')<br>\n",
    "#     b1 = pd.read_csv('submissions_pm/blend1.csv')<br>\n",
    "#     b1f = pd.read_csv('submissions_pm/blend1f.csv')<br>\n",
    "# else:<br>\n",
    "#     sp = pd.read_csv('../submissions_tg/third_gfs.csv')<br>\n",
    "#     s1 = pd.read_csv('../submissions_tg/sat1.csv')<br>\n",
    "#     s2 = pd.read_csv('../submissions_tg/sat2.csv')<br>\n",
    "#     s4 = pd.read_csv('../submissions_tg/sat4.csv')<br>\n",
    "#     a1 = pd.read_csv('../submissions_tg/assim1.csv')<br>\n",
    "#     n = pd.read_csv('../submissions_tg/new.csv')<br>\n",
    "#     st1 = pd.read_csv('../submissions_tg/stack1.csv')<br>\n",
    "#     n2 = pd.read_csv('submissions_tg/new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset == 'pm':<br>\n",
    "#     # plt.scatter(sp.value, out.value, s= 0.1);<br>\n",
    "#     # plt.scatter(sp2.value, out.value, s= 0.1);<br>\n",
    "#     # plt.scatter(g1.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(m1.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(m2.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(m3.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(b1.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(b1f.value, out.value, s= 0.1);<br>\n",
    "# else:<br>\n",
    "#     plt.scatter(sp.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(s1.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(s2.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(s4.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(a1.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(n.value, out.value, s= 0.1);<br>\n",
    "#     plt.scatter(n2.value, out.value, s= 0.1)<br>\n",
    "#     plt.scatter(st1.value, out.value, s= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset == 'pm':<br>\n",
    "#     print(np.corrcoef((g1.value, m1.value, m2.value, m3.value, b1.value, b1f.value, out.value)).round(4))<br>\n",
    "# else:<br>\n",
    "#     print(np.corrcoef((sp.value, s1.value, s2.value, s4.value, a1.value, n.value, n2.value, st1.value, out.value)).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out.datetime < '2021-04-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('submissions_{}'.format(dataset), exist_ok=True)\n",
    "out.to_csv('submissions_{}/new.csv'.format(dataset), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
