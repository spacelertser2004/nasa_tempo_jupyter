{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adamp in /opt/conda/lib/python3.10/site-packages (0.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "!pip install adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = random.choice([#'pm', \n",
    "                         'tg'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP, SGDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc = zstd.ZstdCompressor(level = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open('cache/all_data_{}.pkl'.format(dataset), 'rb'))\n",
    "submission = pickle.load(open('cache/submission_{}.pkl'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle.dump(all_data, open('cache/all_data_{}.pkl'.format(dataset), 'wb'))<br>\n",
    "pickle.dump(submission, open('cache/submission_{}.pkl'.format(dataset), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_data = pickle.loads(s3.get_object(Bucket = 'projects-v', <br>\n",
    "                                     Key = 'aqi/all_data_{}.pkl'.format(dataset) )<br>\n",
    "                                          ['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission = pickle.loads(s3.get_object(Bucket = 'projects-v', <br>\n",
    "                                     Key = 'aqi/submission_{}.pkl'.format(dataset) )<br>\n",
    "                                          ['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17473, 315)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9039, 315)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'tg':\n",
    "    np.random.seed(datetime.datetime.now().microsecond)\n",
    "    all_data.loc[(all_data.grid_id == '7334C') & (np.random.random(len(all_data)) < 0.15), 'grid_id'] = '7F1D1'\n",
    "    all_data.loc[(all_data.grid_id == 'HANW9') & (np.random.random(len(all_data)) < 0.15), 'grid_id'] = 'WZNCR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ids = sorted(all_data.grid_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict = dict(zip(grid_ids, np.arange(len(grid_ids))))\n",
    "# grid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_data[[c for c in all_data.columns if c not in ['datetime', 'value']]].copy()\n",
    "xs = submission[x.columns].copy()\n",
    "y = all_data.value.astype(np.float32)\n",
    "d = all_data.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['dayinyear_sin'] = np.sin(x.dayinyear / 366 * 2 * np.pi)#.plot()\n",
    "x['dayinyear_cos'] = np.cos(x.dayinyear / 366 * 2 * np.pi)#.plot()\n",
    "xs['dayinyear_sin'] = np.sin(xs.dayinyear / 366 * 2 * np.pi)#.plot()\n",
    "xs['dayinyear_cos'] = np.cos(xs.dayinyear / 366 * 2 * np.pi)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold():\n",
    "    def __init__(self, n_splits=5, gap = 30):\n",
    "        self.n_splits = n_splits\n",
    "        self.gap = gap\n",
    "        \n",
    "    def get_n_splits(self, X, y = None, groups = None): return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = groups.sort_values()\n",
    "        X = X.reindex(groups.index)# sort_values(groups)\n",
    "        y = y.reindex(X.index);\n",
    "                     \n",
    "        X, y, groups = sklearn.utils.indexable(X, y, groups)\n",
    "        indices = np.arange(len(X))\n",
    "        \n",
    "        n_splits = self.n_splits\n",
    "        for i in range(n_splits):\n",
    "            test = indices[ i * len(X) // n_splits: (i + 1) * len(X) // n_splits ]#.index\n",
    "            train = indices[ (groups <= groups.iloc[test].min() - datetime.timedelta(days = self.gap) )\n",
    "                          | (groups >= groups.iloc[test].max() + datetime.timedelta(days = self.gap) ) ]#.index\n",
    "            yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedPurgedKFold():\n",
    "    def __init__(self, n_splits = 5, n_repeats = 1, gap = None):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_repeats = n_repeats\n",
    "        self.gap = gap\n",
    "        \n",
    "    def get_n_splits(self, X, y = None, groups = None): \n",
    "        return self.n_splits * self.n_repeats + self.n_repeats * ( self.n_repeats - 1) // 2\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for i in range(self.n_repeats):\n",
    "            for f in PurgedKFold(self.n_splits + i, gap = self.gap if self.gap else None).split(X, y, groups):\n",
    "                yield f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirDataset(Dataset):\n",
    "    def __init__(self, x_loc, g_loc, y_loc, idxs, feature_drops = [] ):\n",
    "        self.x = x_loc.iloc[idxs].drop(columns = feature_drops)\n",
    "        self.g = g_loc.iloc[idxs]\n",
    "        self.y = y_loc.iloc[idxs]\n",
    "    def __getitem__(self, i):\n",
    "        return self.g.iloc[i], self.x.iloc[i].values.astype(np.float32), self.y.iloc[i]\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class path(nn.Module):\n",
    "    def __init__(self, dims, lr, grid_dims, gn, input_dropout, dropout, ):\n",
    "        super().__init__()\n",
    "        self.gn = gn\n",
    "        self.GridEmbedding = nn.Embedding(len(grid_dict), grid_dims);\n",
    "        self.dropout0 = nn.Dropout(input_dropout)\n",
    "        self.linear1 = nn.Linear(x_loc.shape[1] + grid_dims, dims, bias = False)\n",
    "        self.bn1 = nn.GroupNorm(8, dims)\n",
    "        self.a1 = nn.RReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dims, dims, bias = False)\n",
    "        self.bn2 = nn.GroupNorm(8, dims)\n",
    "        self.a2 = nn.RReLU()\n",
    "    def forward(self, g, x):\n",
    "        g = self.GridEmbedding(g) * 0\n",
    "        x = torch.cat((x, g), dim = 1)\n",
    "        if self.training: x += torch.randn(x.shape) * self.gn\n",
    "        x = self.a1( self.bn1( self.linear1( self.dropout0( x ))))\n",
    "        if self.training: x += torch.randn(x.shape) * self.gn\n",
    "        x = self.a2( self.bn2( self.linear2( self.dropout1( x )))) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirModel(pl.LightningModule):\n",
    "    def __init__(self, dims = 128, lr = 0.25, \n",
    "                 grid_dims = 8, gn = 0.1,\n",
    "                 input_dropout = 0.2, dropout = 0.5,\n",
    "                num_paths = 3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.gn = gn\n",
    "        self.GridEmbedding = nn.Embedding(len(grid_dict), grid_dims);\n",
    "        self.paths = nn.ModuleList([path(dims, lr, grid_dims, gn,\n",
    "                                            input_dropout, dropout)\n",
    "                                    for i in range(num_paths)])\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.linear3 = nn.Linear(dims * num_paths, dims, bias = False)\n",
    "        self.bn3 = nn.GroupNorm(8, dims)\n",
    "        self.a3 = nn.PReLU()\n",
    "        self.final_dropout = nn.Dropout(dropout)\n",
    "        self.final_linear = nn.Linear(dims * num_paths + grid_dims, 1)\n",
    "    def forward(self, g, x):\n",
    "        x = torch.cat([p.forward(g, x) for p in self.paths], dim = 1)\n",
    "        # x = self.a3( self.bn3( self.linear3( self.dropout2( x )))) \n",
    "        g = self.GridEmbedding(g)\n",
    "        # if self.training: x += torch.randn(x.shape) * self.gn\n",
    "        # x = self.dropout2(x)\n",
    "        x = torch.cat((x, g), dim = 1)\n",
    "        x = self.final_linear(self.final_dropout( x ))\n",
    "        return x[:, 0]\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.y = []; self.yp = []\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        g, x, y = batch\n",
    "        yp = self.forward(g, x) #* yscale\n",
    "        self.y.append(y); self.yp.append(yp)\n",
    "        loss = nn.MSELoss()(yp, y)\n",
    "        return loss\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g, x, y = batch\n",
    "        yp = self.forward(g, x) #* yscale\n",
    "        # print(yp[:4], y[:4])\n",
    "        loss = nn.MSELoss()(yp, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    def on_validation_epoch_end(self):\n",
    "        y = torch.cat(self.y); yp = torch.cat(self.yp)\n",
    "        loss = nn.MSELoss()(yp, y) ** 0.5\n",
    "        print(loss)\n",
    "        # self.log('val_loss', loss)\n",
    "    def configure_optimizers(self):\n",
    "        return AdamP(self.parameters(), \n",
    "                                lr = learning_rate,\n",
    "                                weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePreds(model_path, model_str):\n",
    "    for e in range(5-1, max_epochs, 5):\n",
    "        model_file = '{}{}-epoch={:02d}.ckpt'.format(\n",
    "                            model_path, model_str, e)\n",
    "        model = AirModel.load_from_checkpoint(model_file)\n",
    "        model.eval();\n",
    "        \n",
    "        if 'full' not in model_file: os.remove(model_file)\n",
    "        val_preds = []; val_y = []; test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for g, x, y in val_loader:\n",
    "                val_preds.append(model(g, x).numpy())\n",
    "                val_y.append(y.numpy())\n",
    "            for g, x, y in test_loader:\n",
    "                test_preds.append(model(g, x).numpy())\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        test_preds = np.concatenate(test_preds)\n",
    "        test_preds = pd.Series(test_preds, test_dataset.x.index)\n",
    "        val_preds = pd.Series(val_preds, val_dataset.x.index)\n",
    "        if 'fold' in model_str:\n",
    "            print(mean_squared_error(val_dataset.y, val_preds) ** 0.5)\n",
    "            with open(model_file.replace('.ckpt', '-val.pkl.zstd'), 'wb') as f:\n",
    "                f.write(zc.compress(pickle.dumps(val_preds)))            \n",
    "        \n",
    "        with open(model_file.replace('.ckpt', '-test.pkl.zstd'), 'wb') as f:\n",
    "            f.write(zc.compress(pickle.dumps(test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!jupyter nbconvert --to script 'TrainNN.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(500):\n",
    "    # Location / Data\n",
    "    # location = random.choice(['Delhi', 'Delhi',\n",
    "    #                        'Los Angeles (SoCAB)', 'Taipei'\n",
    "    #                      ])\n",
    "    location = 'Los Angeles (SoCAB)'\n",
    "    \n",
    "    x_loc = x[x.location == location].drop(columns = ['location', 'grid_id'])\n",
    "    y_loc = y.reindex(x_loc.index)\n",
    "    d_loc = d.reindex(x_loc.index)\n",
    "    g_loc = x.reindex(x_loc.index).grid_id.map(grid_dict).astype(np.int32)\n",
    "\n",
    "    xs_loc = xs[xs.location == location].drop(columns = ['location', 'grid_id'])\n",
    "    gs_loc = xs.reindex(xs_loc.index).grid_id.map(grid_dict).astype(np.int32)\n",
    "    ys_loc = pd.Series(np.nan, index = xs_loc.index)\n",
    "    \n",
    "    # Params\n",
    "    dims = random.choice([64, 128 if location == 'Delhi' else 64 ])\n",
    "    gn = random.choice([0, 0.1, ])\n",
    "    learning_rate = random.choice([3e-4, 1e-3,])\n",
    "    weight_decay = random.choice([1e-4, 1e-3, 1e-2,])\n",
    "    \n",
    "    \n",
    "    folds = list(PurgedKFold(random.choice([4, 5, 6]) if dataset == 'pm' \n",
    "                                    else random.choice([3, 4, 5]),\n",
    "                                gap = random.randrange(20, 60)\n",
    "                            ).split(x_loc, y_loc, d_loc)) \n",
    "    #print('{} folds'.format(len(folds)))\n",
    "    folds += [(np.arange(0, len(x_loc)), [])] \n",
    "\n",
    "    seed = datetime.datetime.now().microsecond\n",
    "    random.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn1/tg_Los_Angeles_(SoCAB)/\n",
      "64dims_gn0.1_lr0.001_wd0.01_run767501_fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/sagemaker-user/nn1/tg_Los_Angeles_(SoCAB) exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | GridEmbedding | Embedding  | 448   \n",
      "1 | paths         | ModuleList | 103 K \n",
      "2 | dropout2      | Dropout    | 0     \n",
      "3 | linear3       | Linear     | 16.4 K\n",
      "4 | bn3           | GroupNorm  | 128   \n",
      "5 | a3            | PReLU      | 1     \n",
      "6 | final_dropout | Dropout    | 0     \n",
      "7 | final_linear  | Linear     | 273   \n",
      "---------------------------------------------\n",
      "120 K     Trainable params\n",
      "0         Non-trainable params\n",
      "120 K     Total params\n",
      "0.483     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.8545)\n",
      "tensor(11.5467)\n",
      "tensor(9.6031)\n",
      "tensor(8.7000)\n",
      "tensor(9.0023)\n",
      "tensor(8.4939)\n",
      "tensor(8.3006)\n",
      "tensor(8.5053)\n",
      "tensor(8.2961)\n",
      "tensor(8.2647)\n",
      "tensor(8.1782)\n",
      "tensor(8.2285)\n",
      "tensor(7.9690)\n",
      "tensor(7.9053)\n",
      "tensor(8.1619)\n",
      "tensor(8.3435)\n",
      "tensor(8.5640)\n",
      "tensor(8.3422)\n",
      "tensor(7.9622)\n",
      "tensor(8.3025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.2671)\n",
      "8.493873632343876\n",
      "8.178174481007169\n",
      "8.343513171088832\n",
      "8.267114631246615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/sagemaker-user/nn1/tg_Los_Angeles_(SoCAB) exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | GridEmbedding | Embedding  | 448   \n",
      "1 | paths         | ModuleList | 103 K \n",
      "2 | dropout2      | Dropout    | 0     \n",
      "3 | linear3       | Linear     | 16.4 K\n",
      "4 | bn3           | GroupNorm  | 128   \n",
      "5 | a3            | PReLU      | 1     \n",
      "6 | final_dropout | Dropout    | 0     \n",
      "7 | final_linear  | Linear     | 273   \n",
      "---------------------------------------------\n",
      "120 K     Trainable params\n",
      "0         Non-trainable params\n",
      "120 K     Total params\n",
      "0.483     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64dims_gn0.1_lr0.001_wd0.01_run767501_fold1\n",
      "tensor(20.6093)\n",
      "tensor(12.2015)\n",
      "tensor(10.2277)\n",
      "tensor(9.9261)\n",
      "tensor(8.9592)\n",
      "tensor(9.5861)\n",
      "tensor(9.2939)\n",
      "tensor(9.4759)\n",
      "tensor(9.2446)\n",
      "tensor(9.1792)\n",
      "tensor(8.9163)\n",
      "tensor(9.0892)\n",
      "tensor(8.9473)\n",
      "tensor(9.4433)\n",
      "tensor(9.0628)\n",
      "tensor(9.3162)\n",
      "tensor(9.3076)\n",
      "tensor(9.2207)\n",
      "tensor(9.5856)\n",
      "tensor(8.7457)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.1869)\n",
      "9.586076780518304\n",
      "8.916295219123821\n",
      "9.316208478639819\n",
      "9.18686305322927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/sagemaker-user/nn1/tg_Los_Angeles_(SoCAB) exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | GridEmbedding | Embedding  | 448   \n",
      "1 | paths         | ModuleList | 103 K \n",
      "2 | dropout2      | Dropout    | 0     \n",
      "3 | linear3       | Linear     | 16.4 K\n",
      "4 | bn3           | GroupNorm  | 128   \n",
      "5 | a3            | PReLU      | 1     \n",
      "6 | final_dropout | Dropout    | 0     \n",
      "7 | final_linear  | Linear     | 273   \n",
      "---------------------------------------------\n",
      "120 K     Trainable params\n",
      "0         Non-trainable params\n",
      "120 K     Total params\n",
      "0.483     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64dims_gn0.1_lr0.001_wd0.01_run767501_fold2\n",
      "tensor(39.1116)\n",
      "tensor(11.1243)\n",
      "tensor(9.6166)\n",
      "tensor(8.8924)\n",
      "tensor(8.8122)\n",
      "tensor(8.9678)\n",
      "tensor(9.3112)\n",
      "tensor(8.6892)\n",
      "tensor(8.9194)\n",
      "tensor(8.6667)\n",
      "tensor(10.0421)\n",
      "tensor(10.0020)\n",
      "tensor(9.5874)\n",
      "tensor(9.6363)\n",
      "tensor(9.8394)\n",
      "tensor(10.2139)\n",
      "tensor(10.6643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.2775)\n",
      "9.863644021793966\n",
      "9.74491867289852\n",
      "9.587369605101506\n",
      "10.277471062376593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/sagemaker-user/nn1/tg_Los_Angeles_(SoCAB) exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | GridEmbedding | Embedding  | 448   \n",
      "1 | paths         | ModuleList | 103 K \n",
      "2 | dropout2      | Dropout    | 0     \n",
      "3 | linear3       | Linear     | 16.4 K\n",
      "4 | bn3           | GroupNorm  | 128   \n",
      "5 | a3            | PReLU      | 1     \n",
      "6 | final_dropout | Dropout    | 0     \n",
      "7 | final_linear  | Linear     | 273   \n",
      "---------------------------------------------\n",
      "120 K     Trainable params\n",
      "0         Non-trainable params\n",
      "120 K     Total params\n",
      "0.483     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64dims_gn0.1_lr0.001_wd0.01_run767501_full\n",
      "tensor(36.6503)\n",
      "tensor(10.2370)\n",
      "tensor(8.3181)\n",
      "tensor(8.1646)\n",
      "tensor(7.9391)\n",
      "tensor(7.7000)\n",
      "tensor(7.8669)\n",
      "tensor(7.6261)\n",
      "tensor(7.7224)\n",
      "tensor(8.1203)\n",
      "tensor(7.8080)\n",
      "tensor(7.5094)\n",
      "tensor(7.6837)\n",
      "tensor(7.8159)\n",
      "tensor(7.5189)\n",
      "tensor(7.3883)\n",
      "tensor(7.5362)\n",
      "tensor(7.5004)\n",
      "tensor(7.6383)\n",
      "tensor(7.4536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5850)\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_fold, test_fold) in enumerate(folds):\n",
    "    # yscale = y_loc.iloc[folds[0][0]].std()\n",
    "\n",
    "    l = random.randrange(0, len(train_fold)//random.randrange(5, 20))\n",
    "    s = random.randrange(0, len(train_fold) - l)\n",
    "    train_fold = train_fold[:s].tolist() + train_fold[s + l:].tolist()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_loc.iloc[train_fold])\n",
    "    x_scaled = pd.DataFrame(scaler.transform(x_loc),\n",
    "                             x_loc.index, x_loc.columns)\n",
    "    xs_scaled = pd.DataFrame(scaler.transform(xs_loc),\n",
    "                             xs_loc.index, xs_loc.columns)\n",
    "\n",
    "    train_dataset = AirDataset(x_scaled, g_loc, y_loc, train_fold)\n",
    "    val_dataset = AirDataset(x_scaled, g_loc, y_loc, test_fold if len(test_fold) > 0 else train_fold)\n",
    "    test_dataset = AirDataset(xs_scaled, gs_loc, ys_loc, np.arange(0, len(xs_scaled)))\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 12, \n",
    "                              shuffle = True, num_workers = os.cpu_count(),\n",
    "                              drop_last = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 256, \n",
    "                              shuffle = False, num_workers = os.cpu_count(),\n",
    "                              drop_last = False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 256, \n",
    "                              shuffle = False, num_workers = os.cpu_count(),\n",
    "                              drop_last = False)\n",
    "\n",
    "\n",
    "    model = AirModel(input_dropout = 0.5,  grid_dims = 16, gn = gn, \n",
    "                         lr = 0.2, dims = dims,\n",
    "                        num_paths = 4)\n",
    "    model_path = 'nn1/{}_{}/'.format(\n",
    "                                dataset, location.replace(' ', '_'))\n",
    "    if fold_idx == 0: print(model_path)\n",
    "    model_str = '{}dims_gn{}_lr{}_wd{}_run{}'.format(\n",
    "                        dims, gn, learning_rate, weight_decay,\n",
    "                            seed) + (\n",
    "                '_fold{}'.format(fold_idx) if fold_idx < len(folds) - 1\n",
    "                    else '_full'); print(model_str)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs = max_epochs, #enable_checkpointing = False, \n",
    "                         logger = False,\n",
    "                                enable_progress_bar = False,\n",
    "                         callbacks = [ pl.callbacks.ModelCheckpoint(\n",
    "                             every_n_epochs = 5, save_top_k = 10,\n",
    "                             monitor = 'val_loss',\n",
    "                            dirpath = model_path,\n",
    "        filename = model_str +  \"-{epoch:02d}\",)])\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    savePreds(model_path, model_str)\n",
    "# break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
